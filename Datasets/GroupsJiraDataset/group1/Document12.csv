id,love,joy,surprise,anger,sadness,fear,notes,comment N,comment N-1,comment N-2,...,comment 2,comment1,,,,,,,,,,,,,,,,,,
68539,,,,,,,I consider the 'thank you' at the end as part of the signature thus not expressing any feeling,Please be informed that I am no longer accessible through this email address. Pls refrain from sending further messages. I will just personally contact you with my new email address. For any concerns / issues / inputs relating to your transactions with Exist; kindly forward email to hr@exist.com. Thank you HR Dept,It appears the servicemix-eip tests were failing when no additional memory was reserved for the unit tests (currently: -Xmx512m). With Adrian's centralized surefire configuration in place; all these tests now work fine. I have removed the <exclusion/>s from the pom.xml (cfr. http://svn.apache.org/viewvc?view=rev&revision=555652),,,,,,,,,,,,,,,,,,,,,,
74046,x,x,,,,,,Looks good; the other services had the type hinting too; so this was indeed an erroneous situation. Applied & Committed; thanks!,attached a patch for fixing this issue. I do not have a working version of the samplecontainer available; so it would be great if someone could check if it still works with the samplecontainer before commiting.,,,,,,,,,,,,,,,,,,,,,,
49721,,,,,,,,It's actually been in there (in jakarta.apache.org/tapestry/current and in CVS) for a while.,Is this the right place for this request? If not; could someone let me know the proper channel? Thanks... Chris,,,,,,,,,,,,,,,,,,,,,,
16908,x,,,,,,,Thanks. The problem is with deferred node expansion; when this is on the Node appears as null instead of a Text node with an empty String. I'll merge a fix shortly.,,,,,,,,,,,,,,,,,,,,,,,
2987,x,,,,,,,Thanks for the patch,The patch ;),,,,,,,,,,,,,,,,,,,,,,
6272,,,,,,,,The problem is not in the SAX parser. The problem occurs if you give the -server flag to the jvm (jdk 1.4/1.3) e.g. java -server <sax parser class> on a really big file. Will cause a stackOverflow. Greg Meagher,,,,,,,,,,,,,,,,,,,,,,,
76425,,,,,,,,This was done long ago and is now in trunk and ready for release,I have merged this work to the trunk. Here's the commit: http://svn.apache.org/viewvc?view=revision&revision=1172075,The patch looks good. Please go ahead and commit.,This patch includes Template Code addition as a result of new design changes.This patch contains both WeblogThemeAssoc and WeblogTemplateCode changes with a test for WeblogTemplateCode.,WeblogThemeAssoc.patch includes new design changes of adding WeblogThemeAssoc class for keeping the association of a weblog and its themes of different types. Please note that this patch is to review the changes and should apply to roller_mobile branch.,This initial patch will enable a user to define a mobile theme and then it will render the set up theme in a mobile device. please create a dummy theme (simply copy paste exisiting one in weblogger-webapp/src/main/wepapp/themes/ and add an entry in theme.xml to say it is a mobile theme. like <type>mobile</mobile>. I have made fauxcoly theme to recognize as a mobile theme. Please note this is a test patch to verify the functionality.,,,,,,,,,,,,,,,,,,
53257,x,,,,,,,Thanks for the patch; Michael. Applied with a few modifications.,Fix for typos attached.,,,,,,,,,,,,,,,,,,,,,,
60912,,,,,,,comment N and N-1 are the same,[branch_4x commit] Mark Robert Miller http://svn.apache.org/viewvc?view=revision&revision=1384969 SOLR-3527: SolrCmdDistributor drops some of the important commit attributes (maxOptimizeSegments; softCommit; expungeDeletes) when sending a commit to replicas.,[branch_4x commit] Mark Robert Miller http://svn.apache.org/viewvc?view=revision&revision=1384969 SOLR-3527: SolrCmdDistributor drops some of the important commit attributes (maxOptimizeSegments; softCommit; expungeDeletes) when sending a commit to replicas.,I was investigating this issue and found that DistribUpdateProcessor distributes the commit commands using the SolrCommandDistributor.distribCommit(...). The method creates Actions to be distributed to other nodes like:  void addCommit(UpdateRequestExt ureq; CommitUpdateCommand cmd) {     if (cmd == null) return;     ureq.setAction(cmd.optimize ? AbstractUpdateRequest.ACTION.OPTIMIZE         : AbstractUpdateRequest.ACTION.COMMIT; false; cmd.waitSearcher);   }  In that method; the action is not considering the 'maxOptimizeSegments' parameter; that's why it's not being distributed to other nodes. However; a bigger problem may be that this method is omitting other parameters too; like 'softCommit' and 'epungeDeletes'; which means that an explicit soft commit issued like:  http://host:port/solr/update?commit=true&softCommit=true  can be distributed as a hard commit to the nodes. I'm not sure about this because I haven't write any test case yet; but it's definitely something to test.,Sounds right Andy - thanks for the report.,One additional data point: the distrib=false does not matter with current behavior. It seems if distrib=false only the local server should be optimized (to the requested value) and if distrib=true (default) all shards in the index should be optimized with N max segments.,,,,,,,,,,,,,,,,,,,
7278,x,,,,,,,Hi George; I just checked in a fix for the empty stack exception. Can you please verify? Thanks.,Hello David; DOMPrint from xml-xerces_20041002043233.tar.gz correctly validates the xml file above. I probably mixed something up. I guess the remaining issue is that this file (from ticket xercesc-1276) should not fail validation against the schema above: <?xml version='1.0'?> <cat> <x:book xmlns:xsi='http://www.w3.org/2001/XMLSchema-instance' xsi:schemaLocation='urn:book sl.xsd' xmlns:x='urn:book' id='bk101'> <x:author>Gambardella; Matthew</x:author> åÊåÊåÊåÊåÊåÊ<x:title>XML Developer's Guide</x:title> åÊåÊåÊåÊåÊåÊ<x:genre>Computer</x:genre> åÊåÊåÊåÊåÊåÊ<x:price>44.95</x:price> åÊåÊåÊåÊåÊåÊ<x:publish_date>2000-10-01</x:publish_date> åÊåÊåÊåÊåÊåÊ<x:description>An in-depth look at creating applications with åÊåÊåÊåÊåÊåÊXML.</x:description> </x:book> </cat>,Hi George; I just tried it out using the latest source and got: Error at file 'd:\bugs\1282.xml'; line 4; column 16 åÊåÊåÊMessage: Unknown element 'x:authors' Error at file 'd:\bugs\1282.xml'; line 11; column 10 åÊåÊåÊMessage: Element 'x:authors' is not valid for content model: '((author;title;genre;price;publish_date);description)' BTW; The empty exception message you get with the file in 1276 (that has cat) is a problem and I am working on fixing that. David,Authors is not in the schema so validation should fail but it doesn't. Here is my command line: domprint -v=always -n -f -s instance.xml,,,,,,,,,,,,,,,,,,,,
57161,,,,,,,,The tests have been added; so closing. Travis; please keep an eye on the results of the tests in our nightly builds and post patches for any build issues. We also need to open a new issue for each test that fails at runtime as a result of thread safety bug(s) in the library; in addition to those for any other failures.,,,,,,,,,,,,,,,,,,,,,,,
26184,,,,,,,,patch applied.,If a directive implementation sees it's parameters are constant; it could evaluate them at initialization time; and substitute a more efficient version for evaluation at runtime.,A simple patch; don't mind applying it; but I'm not sure I understand why this is needed. Can you give a use case when this would be helpful?,,,,,,,,,,,,,,,,,,,,,
37398,,,,,,,,More done: -absolute-ordering faces-config/factory/view-declaration-language-factory faces-config/render-kit/renderer/client-behavior-renderer faces-config/application/partial-traversal,JAR ordering + <name> DONE,,,,,,,,,,,,,,,,,,,,,,
33497,,,,,,,,Closing because this has been in RESOLVED state for over one year; if it turns out to not be fixed please reopen.,,,,,,,,,,,,,,,,,,,,,,,
60365,,,,,,,,Fix committed as part of SOLR-4134.,Here's a patch (branch_4x) to reproduce the problem. I'm working on the fix.,Creating a CloudSolrServer and setting it this custom LBHttpSolrServer solves the problem: import org.apache.solr.client.solrj.impl.BinaryRequestWriter; import org.apache.solr.client.solrj.impl.HttpSolrServer; import org.apache.solr.client.solrj.impl.LBHttpSolrServer; package com.example.custom.solr; import java.net.MalformedURLException; public class BinaryLBHttpSolrServer extends LBHttpSolrServer { private static final long serialVersionUID = 3905956120804659445L; public BinaryLBHttpSolrServer(String[] endpoints) throws MalformedURLException { super(endpoints); } @Override protected HttpSolrServer makeServer(String server) throws MalformedURLException { HttpSolrServer solrServer = super.makeServer(server); solrServer.setRequestWriter(new BinaryRequestWriter()); return solrServer; } },Iå«ve noticed that the same occurs using HttpSolrServer. However; if you set to HttpSolrServer instance the following Writers it works: HttpSolrServer solrServer = new HttpSolrServer(...); solrServer.setParser(new BinaryResponseParser()); solrServer.setRequestWriter(new BinaryRequestWriter()); Iå«ve tried to find a way to set those Writers via CloudServer or via LBHttpSolrServer but I found nothing.,,,,,,,,,,,,,,,,,,,,
7636,,x,,,,,,As far as I see tthis seems to be fixed in at least Version 2.6.0. Therefore this issue can be closed!,Following value was generated with IDE of BCB6 UpdatePack4 (Project -> Export Makefile...): PATHCPP = .;..\..\..\..\..\src\xercesc\util\Platforms\Win32;..\..\..\..\..\src\xercesc\util\MsgLoaders\Win32;..\..\..\..\..\src\xercesc\util\Transcoders\Win32;..\..\..\..\..\src\xercesc\util\NetAccessors\WinSock;..\..\..\..\..\src\xercesc\util\regx;..\..\..\..\..\src\xercesc\util;..\..\..\..\..\src\xercesc\framework;..\..\..\..\..\src\xercesc\internal;..\..\..\..\..\src\xercesc\sax;..\..\..\..\..\src\xercesc\parsers;..\..\..\..\..\src\xercesc\validators\common;..\..\..\..\..\src\xercesc\validators\datatype;..\..\..\..\..\src\xercesc\validators\DTD;..\..\..\..\..\src\xercesc\validators\schema\identity;..\..\..\..\..\src\xercesc\validators\schema;..\..\..\..\..\src\xercesc\sax2;..\..\..\..\..\src\xercesc\dom\impl;..\..\..\..\..\src\xercesc\dom\deprecated;..\..\..\..\..\src\xercesc\dom It is slightly different from yours but has no duplicated pathes. Can't imagine how the original value was generated.,,,,,,,,,,,,,,,,,,,,,,
29936,x,,,,,,,To test this; I used an aggregate AE with a CAS multiplier that declared getCasInstancesRequired()=5. If this AE is instantiated and run in a loop with earlier code it eats up roughly 10MB per iteration. No such leak with the latest code. Thanks!,I believe I've fixed this leak. Eddie; can you verify?,,,,,,,,,,,,,,,,,,,,,,
34239,,,,,,,,Fixed in revision 406707.,This patch deletes the existing overview file and redirects the links to the Wiki.,,,,,,,,,,,,,,,,,,,,,,
37251,,,,,,,,This issue is already comitted,patch file attached for Trunk,,,,,,,,,,,,,,,,,,,,,,
28313,x,,x,,,,,Thanks for that comment Richard! I was laboring under the (false) impression that I could not change the docs that went with our previous releases; but I see these are coming from a copy on our svn site; and we could actually change that one. Anybody object to my doing that? I could correct the link; and also indicate this release is 2 levels out of date.,It might be a good idea to add a prominent notice to the old documentation pages that the documentation is outdated and also add such a not to the outdated update site links.,I'm not sure how to 'fix' this. The reference url to the documentation is for an old release and the link was valid while we were in the incubator; but now that we've graduated; we were required to remove our old distributions there. The new documentation (for releases since we've graduated) has the correct link. Does anyone know if there's some kind of 'forwarding' link that can work for eclipse download sites? Since this is mirrored; it would be good if the forwarding mechanism also worked on the mirrors...,,,,,,,,,,,,,,,,,,,,,
27475,,x,,x,,,"joy is for ""the fix is pretty simple""; anger is for ""the implementation of that isn't very beatuful""",svn ci -m 'Fixing ArrayOutOfBoundsException bug reported in # VALIDATOR-202. This fix is pretty simple - 'Create an array of size # of .s plus one'; but the implementation of that isn't very beautiful. ' Sending src/share/org/apache/commons/validator/UrlValidator.java Sending src/test/org/apache/commons/validator/UrlTest.java Transmitting file data .. Committed revision 453308.,See VALIDATOR-203 for Jira issue to RECODE ME.,ArrayList seemed like a 'doh' moment; but digging into it it's a bit of a pain to code. You can't use set as you have to use add the first time in an ArrayList; so the code would need a large change. When I was looking at this class there were a number of bits that were jumping up and down saying 'RECODE ME!'; so I'm going to apply the two patches above to get the bug fixed in the nightly build tonight and then I'll look into rewriting things a bit. ArrayList would work fine if the code was changed from set(..) to add(..) semantics; but I also think the whole thing could be simplified.,Please ignore the previous comment as to where I can get the fix. I did not see the attached patch file.,How can I get your fix? Also you can always use an ArrayList instead of fixed size array so it wont seem so hacky.,Problem is that in that method an array of fixed size 10 is created. The patch resolves this by making the size of the array be the number of '.'s in the hostIP + 1. Seems hacky,Unit test.,Using commons-validator-1.3.0.jar - Version 1.3.0 of commons validator,,,,,,,,,,,,,,,,
35079,,,,x,,,"I believe the ""thanks uri"" is ironic",46fa49e75e79c4bf7351bbdc0653130f4e01dfb0 TS-1558: use_client_addr breaks control over upstream HTTP protocol version Thanks Uri!,,,,,,,,,,,,,,,,,,,,,,,
80577,,,,,,,,Test log,A thread dump with a deadlock stack traces,,,,,,,,,,,,,,,,,,,,,,
3591,,,,,,,,That is my understanding. Looking at the dtd; it only defines a type attribute (required) on the validator element; which doesn't make any sense for this file.,So validators.xml shouldn't have a DTD - is that correct? Matt,The problem is you shouldn't be using the dtd for this file. The validator dtd is only for actual object and field validations; not validator definitions. I've fixed the unit tests since it also thought it was to use the dtd.,I've seen this issue with both WebWork 2.2.4 and Struts 2.0.1. Maybe it's my validators.xml: <!DOCTYPE validators PUBLIC '-//OpenSymphony Group//XWork Validator 1.0//EN' åÊåÊåÊåÊ'http://www.opensymphony.com/xwork/xwork-validator-1.0.dtd'> <validators> åÊåÊåÊåÊ<validator name='required' class='com.opensymphony.xwork2.validator.validators.RequiredFieldValidator'/> åÊåÊåÊåÊ<validator name='requiredstring' class='com.opensymphony.xwork2.validator.validators.RequiredStringValidator'/> åÊåÊåÊåÊ<validator name='int' class='com.opensymphony.xwork2.validator.validators.IntRangeFieldValidator'/> åÊåÊåÊåÊ<validator name='date' class='com.opensymphony.xwork2.validator.validators.DateRangeFieldValidator'/> åÊåÊåÊåÊ<validator name='expression' class='com.opensymphony.xwork2.validator.validators.ExpressionValidator'/> åÊåÊåÊåÊ<validator name='fieldexpression' class='com.opensymphony.xwork2.validator.validators.FieldExpressionValidator'/> åÊåÊåÊåÊ<validator name='email' class='com.opensymphony.xwork2.validator.validators.EmailValidator'/> åÊåÊåÊåÊ<validator name='url' class='com.opensymphony.xwork2.validator.validators.URLValidator'/> åÊåÊåÊåÊ<validator name='visitor' class='com.opensymphony.xwork2.validator.validators.VisitorFieldValidator'/> åÊåÊåÊåÊ<validator name='conversion' class='com.opensymphony.xwork2.validator.validators.ConversionErrorFieldValidator'/> åÊåÊåÊåÊ<validator name='stringlength' class='com.opensymphony.xwork2.validator.validators.StringLengthFieldValidator'/> </validators>,Are we sure this is a bug in xwork 2.0? I've never come across it...,Toby; isn't this fixed for 1.2.2-dev already and only needs a backport to xwork-2.0? If yes; could you please have a look at this; otherwise close the issue please. tia; Rainer,,,,,,,,,,,,,,,,,,
722,,,,,,,,Out of curiosity; could we save a FOLLOWING notification as our lastMessage then; even if we don't send it? That would give you the 'current' state more accurately than a stale LOOKING notification and it seems like it could solve this particular problem. We could; but that wouldn't give an accurate state as well. A peer might be in the LOOKING state (next FLE round) while the lastMessage gets sent. There will be race conditions. In my opinion; a better way of doing this is to send the current state instead of lastMessage. In which case; I agree with Flavio that sending the final notifications won't be necessary. This will also reduce the number of notifications exchanged.,It is unclear to me why you say that a FOLLOWING notification does not get enqueued; Vishal. A peer does send a notification when it receives one from a peer LOOKING through the same mechanism as other notifications. My comment was in the context of previous scenario that we were discussing.,It is unclear to me why you say that a FOLLOWING notification does not get enqueued; Vishal. A peer does send a notification when it receives one from a peer LOOKING through the same mechanism as other notifications. Also; you're right that it does not send a final notification message. I believe it is this way because the peer will still respond to a notification from a peer that it is looking; so the message is not strictly necessary.,Ok; I didn't see that thanks. Out of curiosity; could we save a FOLLOWING notification as our lastMessage then; even if we don't send it? That would give you the 'current' state more accurately than a stale LOOKING notification and it seems like it could solve this particular problem.,The FOLLOWING notification does not get qeueued. If I am not mistaken; a peer does not send a notification after finishing FLE.However; I think that my patch for 975 would avoid this problem. In that patch I store only the most recently sent notification (and not all notifications). The most recent notification sent by S2 would be the one where it tells everyone that it is LOOKING and it thinks that S1 is the leader. We should get rid of lastMessage in QCM. wherever we send lastMessage; we should send the current state of the peer. So; if the peer is in the middle of FLE; send <epoch;proposedleader; proposedzxid; LOOKING>. Otherwise; send <epich; leader; lastLoggedZxid(); FOLLOWING/LEADING>,Er first sentance is backwards should read: Wouldn't S3 recieve a FOLLOWING notification from S2 not LOOKING if S2 ended up following S1 initially.,Wouldn't S2 receive a FOLLOWING notification from S3; not a LOOKING notification; if S3 ended up following S1 initially? Isn't FOLLOWING the last notification that it sends? In which case T3: S3 starts FLE from e=0 xid=5; sees a FOLLOWING notification; has no quorum; and goes to round 2. T4: Attempt to LEAD from S2 with the higher xid; and cede power. Or is the FOLLOWING message somehow not put on the queue?,If I am not mistaken the scenario Flavio presents is as follows: T0: S1; S2; S3 start FLE. Lets assume that S2's notification is (e=1; xid=1). In the end; S1 wins. T1: S1(e=1; xid=10); S2(e=1; xid=10); S3(e=1; xid=5) T2: S1 and S3 crash T3: S3 reboots (really fast); starts FLE from <e=0; xid=10>; and receives S2's notification of (e=1; xid=1) T4: S3 updates its epoch to 1; compares <e=1; xid=1> vs its <e=1; xid=5> and decides to lead. However; S2 should not follow S3 (after removing the blind following condition). S2 will follow S3 only if a majority are following S3. In this case they are not. Now; S2 in turn will receive a 'LOOKING' notification from S3 (sent during T3). S2 will also decide to lead. Both will think they are LEADING. They will timeout and start another round of election. This time; S2 will turn out to be the winner. Does this make sense? I think the problem here is 'lastMessage' in QCM. I was thinking about this while working on the patch. As I mentioned in 975; I think we should send the current state <e; lastLoggedZxid> instead of lastMessage. Also; in T3; we might want to initialize logicalClock to epoch stored in transaction logs. What do you think?,How would S3 get an old message from S2? I don't understand what you mean by message duplicates in this scenario.,Right; but what if S3 crashes and recovers? In this case; logicalclock is reset and receiving the old message from S2 would cause S3 to lead; as described above. If this is right; then is it only possible if we have message duplicates?,7.S1 drops leadership (doesn't matter why); 8.S3 starts leader election and receives the notification S2 sent in step 3; Wouldn't the notification from S2 have n.epoch < logicalclock; causing it to be ignored?,Hi Flavio; I submitted a patch to ZOOKEEPER-975 yesterday; but I forgot to mention it on this bug. I think it addresses both 975 and this bug. I am not sure if you got a chance to take a look at it? (I just noticed that the patch failed on trunk. I will fix that) The patch removes the two incorrect if conditions that I mentioned above (in addition to changes done for 975). Note - in the diff; for the the second if (where the peer blindly follows); we may have to check the epoch as well. I also posted a question yesterday pointing out a place where we don't send the latest zxid when the cluster is running. I think we should fix that as well. I was planning to write a test to verify the patch; but I will wait to hear back regarding the diff and questions posted to ZOOKEEPER-975 before proceeding.,To follow up on my own comments. Here is how I'm currently thinking. For the issue described in this jira to happen; the following needs to hold: We have a broken leader that does not have the highest last zxid in a quorum; There is a quorum containing the broken leader such that for every element of the quorum; the last zxid epoch is the same or smaller compared to the epoch of the broken leader. If this condition doesn't hold; then at least one server will not follow the broken leader; The broken leader must have received at least one notification during leader election that reflect an old state of the system. Clearly the problem Vishal has reported matches this description; since repeating notifications causes a server to receive notifications that might reflect a stale state of the system. However; I'm not entirely convinced that we can completely get rid of this problem with a patch for ZOOKEEPER-975 because of scenarios like the following; which at least abstractly seems to work: There are three servers: S1; S2; S3; Servers come all from epoch e ; and are trying to elect a new leader for e+1; Each server sends one notification to each of the other servers; proposing itself as leader; S2 and S3 receive a notification from S1; and suppose that S1's vote supersedes their own votes. S2 and S3 eventually decide to follow S1; S1 receives notifications from S2 and S3 and decides to lead; S2 follows S1 for a longer time than S3; so S3 lags behind; S1 drops leadership (doesn't matter why); S3 starts leader election and receives the notification S2 sent in step 3; S3 believes it is the leader and starts leading; S2 receives a leading notification from S3 and starts following S3; thus truncating its log (!!!) The last step is mainly due to the broken 'if' statement I mentioned before. Now; in this example; there was no repetition of notification messages ( ZOOKEEPER-975); and yet we've reached a problem. The question is if this run can happen or not. Can you see any step that can't happen? To me; the one step that is not clear is Step 8; but I can't convince myself that it can't happen.,I'm not entirely convinced that ZOOKEEPER-975 is the right fix for this problem. In my perception; it might make it less probable; but it still doesn't get rid of the problem. Let me put some thoughts here. My understanding is that currently we rely upon a safety property from leader election; which says that the server that arises as leader has the highest zxid among a quorum of servers. This is a nice optimization because it speeds up the recovery process. The problem I see is that the zxid used in votes to determine leadership may not be the same as the current zxids of servers because of the problem that ZOOKEEPER-975 points to or simply because messages come in late. The former is exactly the case we observed here. Consequently; leader election violated the safety property it was supposed to satisfy. My current opinion is that we should be making sure that the leader really has the highest zxid among a quorum before exercising leadership. I believe we do not perform this check today; and this probably implies a deeper change and requires some more thinking. Regarding the 'if' that blindly accepts a leader; I think it should go away. It sounds wrong even without the fix for ZOOKEEPER-975.,That's a good catch. I was thinking that the only thing we might need to do is remove the case that follows a leader blindly. In my understanding; this is the case that allowed server 2 to follow server 3. Server 1 wouldn't have followed server 3 if there wasn't a quorum supporting 3; since the 'if' statement that makes the decision on following the server is guarded by this predicate: (n.epoch == logicalclock) It sounds like we will need a new release soon...,Wow; thanks for the in-depth explanation. It makes sense to me; in terms of the timeline of events and what could go wrong; but I don't know enough about the Zookeeper code to be able to verify for sure. I would love to try out a patch for ZOOKEEPER-975 and see if that fixes the problem for me. (I added myself as a watcher for that bug.),Hi Jeremy; I think I know whats going on here. The bug description that I gave in my earlier comment is causing legitimate transactions to get truncated. You were right about node 3 becoming the leader after reboot. I have given the order of events and my explanation below. Note that the order of events is slightly different from what you described. 1. After your step 6; node 1 becomes the leader (its ID is > node 3's id). Heres what node1 is saying in terms of membership and elections rounds. a) 2048:151124 [QuorumPeer:/0.0.0.0:2888] WARN org.apache.zookeeper.server.quorum.Learner - Exception when following the leader <==== NODE 2 WENT DOWN HERE (your step #6) b) 13497: at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:375) ===> NODE 3 WENT DOWN HERE (your step #8) c) 17926:309591 [LearnerHandler-/13.0.0.12:52753] WARN org.apache.zookeeper.server.quorum.LearnerHandler - ******* GOODBYE /13.0.0.12:52753 ******** ===> NODE2 WENT DOWN HERE AGAIN (Your step #10) d) 18044 310757 [QuorumPeer:/0.0.0.0:2888] INFO org.apache.zookeeper.server.quorum.Leader - Shutdown called ===> NODE1 shutdown again? (This is because it lost majority as explained below) 18045 at org.apache.zookeeper.server.quorum.Leader.shutdown(Leader.java:390) 18046 at org.apache.zookeeper.server.quorum.Leader.lead(Leader.java:367) 18047 at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:658) e) 23685:1831 [pool-1-thread-3] INFO org.apache.zookeeper.server.quorum.QuorumPeer - tickTime set to 3000 ====> NODE1 restarted again (this is because of your client assert) 2. Notice that the time difference between b) and c) above is only 4.5 seconds. So you rebooted node 2 before node 3 had joined the cluster. As a result node 1 lost its majority and gave up leadership. You can see that shutdown is called from Leader.java:367; which is done when the leader cannot ping the followers. Your application would have seen a DISCONNECT before it asserted. What took node 3 so long to join the cluster? As described in ZOOKEEPER-975 and my previous comment; 3 went into LEADING state because of the bug in FastLeaderElection. After 3 reboots; nodes 1 and 2 send old notifications to 3. When 3 receives notifications of all nodes; it goes in the leading state. Heres the incorrect if condition from FastLeaderElection //If have received from all nodes; then terminate if ((self.getVotingView().size() == recvset.size()) && (self.getQuorumVerifier().getWeight(proposedLeader) != 0)) { self.setPeerState((proposedLeader == self.getId()) ? ServerState.LEADING: learningState()); leaveInstance(); return new Vote(proposedLeader; proposedZxid); } 3. Now; 3 is in LEADING state and it will remain in the LEADING state until ticktime * initTime or until a majority of followers start following 3. In the mean time; 2 boots and starts leader election. 2 receives a notification from 3; which claims 3 to be the leader. 2533 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection - Notification: 37 (n.leader); 17179869831 (n.zxid); 3 (n.round); LEADING (n.state); 37 (n.sid); LOOKING (my state) 2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - id: 126; proposed id: 126; zxid: 17179869911; proposed zxid: 17179869911 2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - Adding vote: From = 126; Proposed leader = 126; Porposed zxid = 17179869911; Proposed epoch = 1 2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - id: 37; proposed id: 126; zxid: 17179869831; proposed zxid: 17179869911 2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - Adding vote: From = 37; Proposed leader = 37; Porposed zxid = 17179869831; Proposed epoch = 3 2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - I'm a participant: 126 2 blindly believes that 3 is the leader and starts following 3! As a result; 2's state is rolled-back (because 3 has old zxid). So 2 truncates its transactions logs. Heres the incorrect if statement: default: /* There is at most one leader for each epoch; so if a peer claims to be the leader for an epoch; then that peer must be the leader (no* arbitrary failures assumed). Now; if there is no quorum supporting this leader; then processes will naturally move to a new epoch. */ if(n.epoch == logicalclock) Unknown macro: { recvset.put(n.sid; new Vote(n.leader; n.zxid; n.epoch)); if((n.state == ServerState.LEADING) || (termPredicate(recvset; new Vote(n.leader; n.zxid; n.epoch; n.state)) && checkLeader(outofelection; n.leader; n.epoch)) ){ self.setPeerState((n.leader == self.getId()) ? ServerState.LEADING: learningState()); leaveInstance(); return new Vote(n.leader; n.zxid); } } 3. Now; 1 is also running leader election (it does this immediately after loosing majority). 1 now receives votes from 2 and 3 that say that 3 is the leader. So; 1 starts following 3. 18427 314274 [WorkerReceiver Thread] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - Receive new notification message. My id = 215 18428 314274 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection - Notification: 37 (n.leader); 17179869831 (n.zxid); 3 (n.round); FOLLOWING (n.state); 126 (n.sid); LOOKING (my state) 18429 314274 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - I'm a participant: 215 18430 314274 [QuorumPeer:/0.0.0.0:2888] INFO org.apache.zookeeper.server.quorum.QuorumPeer - FOLLOWING As a result; 1 ends up truncating its transactions as well. In order for your client to see 3292 znode; the ensemble would have to rolled-back at least (3348 - 3291) 57 transactions. We can see that both 1 and 2 went back (17179869911 - 17179869831) 80 transactions. In short; there are several places where a peer goes in correct states. The main reason for this is that it does not always rely on <epoch; id; zxid> to determine leadership. ZOKEEPER-975 had identified some of them; and the fix for ZOKEEPER-975 would have prevented this bug. I already have a working patch for ZOKEEPER-975 that I did not attach to the bug; because I had implemented it on top of ZOOKEEPER-932. To avoid back porting and testing efforts; I was hoping that ZOOKEEPER-932 would get reviewed soon so that I can post a patch for ZOOKEEPER-975. It turns out that ZOOKEEPER-975 is a blocker; so I will pull out the changes for ZOOKEEPER-975 and submit it for review. We might have to do more changes to cleanup FastLeaderElection; but my patch for ZOOKEEPER-975 should at least get rid of the bug that you are seeing. Let me know if you think I have miscalculated things here.,Could it be this: A. Notice that suddently node 2 goes in following state: 2533 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection - Notification: 126 (n.leader); 17179869911 (n.zxid); 1 (n.round); LOOKING (n.state); 126 (n.sid); LOOKING (my state) 2533 [WorkerReceiver Thread] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - Receive new notification message. My id = 126 2533 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection - Notification: 37 (n.leader); 17179869831 (n.zxid); 3 (n.round); LOOKING (n.state); 37 (n.sid); LOOKING (my state) 2533 [WorkerReceiver Thread] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - Receive new notification message. My id = 126 2533 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection - Notification: 37 (n.leader); 17179869831 (n.zxid); 3 (n.round); LEADING (n.state); 37 (n.sid); LOOKING (my state) 2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - id: 126; proposed id: 126; zxid: 17179869911; proposed zxid: 17179869911 2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - Adding vote: From = 126; Proposed leader = 126; Porposed zxid = 17179869911; Proposed epoch = 1 2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - id: 37; proposed id: 126; zxid: 17179869831; proposed zxid: 17179869911 2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - Adding vote: From = 37; Proposed leader = 37; Porposed zxid = 17179869831; Proposed epoch = 3 2534 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - I'm a participant: 126 B. Due to ZOOKEEPER-975; 3 goes in LEADING state because it receives history of old notifications from node 1 and node2. node3_after_restart: 2653 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection - Notification: 37 (n.leader); 17179869831 (n.zxid); 1 (n.round); LOOKING (n.state); 37 (n.sid); LOOKING (my state) //If have received from all nodes; then terminate if ((self.getVotingView().size() == recvset.size()) && (self.getQuorumVerifier().getWeight(proposedLeader) != 0)) { self.setPeerState((proposedLeader == self.getId()) ? ServerState.LEADING: learningState()); leaveInstance(); return new Vote(proposedLeader; proposedZxid); } C. After which node 2 starts following node 3 and then node 1 starts following node 3 (because 2 is following 3) due to: default: /* There is at most one leader for each epoch; so if a peer claims to be the leader for an epoch; then that peer must be the leader (no* arbitrary failures assumed). Now; if there is no quorum supporting this leader; then processes will naturally move to a new epoch. */ if(n.epoch == logicalclock) Unknown macro: { recvset.put(n.sid; new Vote(n.leader; n.zxid; n.epoch)); if((n.state == ServerState.LEADING) || (termPredicate(recvset; new Vote(n.leader; n.zxid; n.epoch; n.state)) && checkLeader(outofelection; n.leader; n.epoch)) ){ self.setPeerState((n.leader == self.getId()) ? ServerState.LEADING: learningState()); leaveInstance(); return new Vote(n.leader; n.zxid); } },The logs are really very difficult to follow. What would make them easier to follow? I am about to replace the relative timestamps with wallclock timestamps and I will try to reproduce. Is there anything else that might help? 2048:151124 [QuorumPeer:/0.0.0.0:2888] WARN org.apache.zookeeper.server.quorum.Learner - Exception when following the leader <==== NODE 2 WENT DOWN HERE 13497: at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:375) ===> NODE 3 WENT DOWN HERE 17926:309591 [LearnerHandler-/13.0.0.12:52753] WARN org.apache.zookeeper.server.quorum.LearnerHandler - ******* GOODBYE /13.0.0.12:52753 ******** ===> NODE2 WENT DOWN HERE AGAIN Correct; node2 dies twice during the period I've described. (Steps #6 and #10.) 18044:310757 [QuorumPeer:/0.0.0.0:2888] INFO org.apache.zookeeper.server.quorum.Leader - Shutdown called ===> NODE1 shutdown again (which is why node 3 became the leader)? This happens because my node1 client code; running in tandem with the ZK server code; hits an assert failure when it detects the sequence numbers decreasing; and kills the server code as well; then our framework restarts everything. So this is AFTER the client creates a file with a decreased seqno. Everything after this point in the node1 file did not contribute to the bug. (I left it in the log in case it showed anything interesting after the restart.) If you grep for 0000000000000000_record0000003292 and 0000000000000000_record0000003348 in node2_after_restart.log we can see that znode ending with 3292 was at least accessed before 3348. Yes; while node2 was alive; nodes up to 3348 were accessed. That 3292 is the original node with that sequence number. The second one created by node1 does not appear to show up in the logs labeled as such _x0089_ÛÒ I'm not sure how to get the ZookeeperServer to print a log message containing the name of the file it created. I am confused to see that node1_after_restart.log does not have any trace of sequential znodes beyond /zkrsm/0000000000000000_record0000000921 (especially because Node1 was the leader when the znodes in questions were created). It looks to me like node #2 was the leader for a large chunk of time based on these lines in node1_after_restart: 2535 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection - Notification: 126 (n.leader); 8589936862 (n.zxid); 2 (n.round); LEADING (n.state); 126 (n.sid); LOOKING (my state) ... 154676 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection - Notification: 215 (n.leader); 12884902548 (n.zxid); 3 (n.round); LOOKING (n.state); 126 (n.sid); LEADING (my state) After that; there are a bunch of create messages within /zkrsm; but no GetDatas. I am also not sure whether to trust the cxid in these messages or not. If I am not mistaken; they represent the current transaction id as seen by the client. How can we isolate the transaction that did the creates? This confused me as well. If there's anything I can do to get more accurate logs; please let me know. I'm not sure if I can reproduce this yet; but I have a strategy I'm planning to try.,If you grep for 0000000000000000_record0000003292 and 0000000000000000_record0000003348 in node2_after_restart.log we can see that znode ending with 3292 was at least accessed before 3348. 2943:153670 [FollowerRequestProcessor:126] TRACE org.apache.zookeeper.server.quorum.FollowerRequestProcessor - :Fsessionid:0x7e2ec782ff5f0000 type:getDate cxid:0x4d830245 zxid:0xfffffffffffffffe \ txntype:unknown reqpath:/zkrsm/0000000000000000_record0000003292 [...] 4340:156963 [FollowerRequestProcessor:126] TRACE org.apache.zookeeper.server.quorum.FollowerRequestProcessor - :Fsessionid:0x7e2ec782ff5f0000 type:getDate cxid:0x4d8302bb zxid:0xfffffffffffffffe \ txntype:unknown reqpath:/zkrsm/0000000000000000_record0000003348 The first column above indicates line number. The type (getDate) is a typo and should be getData. Similarly; in log3_after_restart: 32497:24471 [ProcessThread:-1] TRACE org.apache.zookeeper.server.PrepRequestProcessor - :Psessionid:0x252ec7856ce60000 type:getDate cxid:0x4d8300c1 zxid:0xfffffffffffffffe txntype:unknown reqpath\ :/zkrsm/0000000000000000_record0000003292 [...] 37513:27851 [ProcessThread:-1] TRACE org.apache.zookeeper.server.PrepRequestProcessor - :Psessionid:0x252ec7856ce60001 type:getDate cxid:0x4d8300f3 zxid:0xfffffffffffffffe txntype:unknown reqpath\ :/zkrsm/0000000000000000_record0000003348 I am confused to see that node1_after_restart.log does not have any trace of sequential znodes beyond /zkrsm/0000000000000000_record0000000921 (especially because Node1 was the leader when the znodes in questions were created). I am also not sure whether to trust the cxid in these messages or not. If I am not mistaken; they represent the current transaction id as seen by the client. How can we isolate the transaction that did the creates?,The logs are really very difficult to follow. Also; I think the sequence of events as described do not match with the logs. Heres what we can see in from node1_after_restart 2048:151124 [QuorumPeer:/0.0.0.0:2888] WARN org.apache.zookeeper.server.quorum.Learner - Exception when following the leader <==== NODE 2 WENT DOWN HERE 13497: at org.apache.zookeeper.server.quorum.LearnerHandler.run(LearnerHandler.java:375) ===> NODE 3 WENT DOWN HERE 17926:309591 [LearnerHandler-/13.0.0.12:52753] WARN org.apache.zookeeper.server.quorum.LearnerHandler - ******* GOODBYE /13.0.0.12:52753 ******** ===> NODE2 WENT DOWN HERE AGAIN 18044:310757 [QuorumPeer:/0.0.0.0:2888] INFO org.apache.zookeeper.server.quorum.Leader - Shutdown called ===> NODE1 shutdown again (which is why node 3 became the leader)? There seem to be a few restarts after this as well grep -n 'tickTime set to 3000' node1_after_restart.log [...] 23685:1831 [pool-1-thread-3] INFO org.apache.zookeeper.server.quorum.QuorumPeer - tickTime set to 3000 28491:1479 [pool-1-thread-2] INFO org.apache.zookeeper.server.ZooKeeperServer - tickTime set to 3000 So anything beyond time 310757 is not as described in the bug report. Also note that because of ZOKEEPER-975; after receiving notifications from all peers a node can go in LEADING state until it waits for initTime() * tickTime(). Then; it will timeout; start FLE; and go to FOLLOWING state.,These logs are very difficult to read; but one thing I notice is this log from node1_after_restart: 314258 [Thread-3] DEBUG org.apache.zookeeper.server.quorum.QuorumCnxManager - Address of remote peer: 126 314267 [WorkerReceiver Thread] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - Receive new notification message. My id = 215 314267 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection - Notification: 126 (n.leader); 17179869911 (n.zxid); 1 (n.round); LOOKING (n.state); 126 (n.sid); LOOKING (my state) 314267 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - Notification epoch is smaller than logicalclock. n.epoch = 1; Logical clock4 314267 [WorkerSender Thread] DEBUG org.apache.zookeeper.server.quorum.QuorumCnxManager - There is a connection already for server 126 314273 [WorkerReceiver Thread] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - Receive new notification message. My id = 215 314273 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection - Notification: 126 (n.leader); 17179869911 (n.zxid); 3 (n.round); LOOKING (n.state); 126 (n.sid); LOOKING (my state) 314273 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - Notification epoch is smaller than logicalclock. n.epoch = 3; Logical clock4 314273 [WorkerSender Thread] DEBUG org.apache.zookeeper.server.quorum.QuorumCnxManager - There is a connection already for server 126 314274 [WorkerReceiver Thread] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - Receive new notification message. My id = 215 314274 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection - Notification: 37 (n.leader); 17179869831 (n.zxid); 3 (n.round); FOLLOWING (n.state); 126 (n.sid); LOOKING (my state) 314274 [QuorumPeer:/0.0.0.0:2888] DEBUG org.apache.zookeeper.server.quorum.FastLeaderElection - I'm a participant: 215 QuorumPeer:/0.0.0.0:2888] INFO org.apache.zookeeper.server.quorum.QuorumPeer - FOLLOWING Specifically; that the n.zxid starts at 17179869911; and goes to 17179869831 when election finishes. And in fact node3 does believe it is LEADING according to the logs. It's also confusing because the node3 logs have a bunch of lines like this: 6397 [WorkerReceiver Thread] INFO org.apache.zookeeper.server.quorum.FastLeaderElection - Notification: 215 (n.leader); 17179869918 (n.zxid); 4 (n.round); LOOKING (n.state); 215 (n.sid); LEADING (my state) Why is my state LEADING; but n.leader is node1; but node1 is also LOOKING?,Actually; step 11 above is not correct; according to Flavio on the mailing list. Node #3 proposed that it become leader; but did not have enough followers.,Contains a partial before-restart log for node #3 (node3_after_restart.log) and an after-restart log for all three nodes.
644,,,,,,,,Integrated in ZooKeeper-trunk #1266 (See https://builds.apache.org/job/ZooKeeper-trunk/1266/) ZOOKEEPER-1104. CLONE - In QuorumTest; use the same 'for ( .. try { break } catch { } )' pattern in testFollowersStartAfterLeaders as in testSessionMove. (Eugene Koontz via mahadev) mahadev : http://svn.apache.org/viewcvs.cgi/?root=Apache-SVN&view=rev&rev=1157690 Files : /zookeeper/trunk/src/java/test/org/apache/zookeeper/test/QuorumTest.java /zookeeper/trunk/CHANGES.txt,I just pushed this to trunk only! Thanks Eugene; do let me know if this needs to go into 3.3 branch.,Eugene; I am assuming the patch is just for trunk. Let me know if it needs to go into 3.3 branch as well.,Hi Mahadev; Please use the ZOOKEEPER-1104 one ( ZOOKEEPER-1104.patch) -Eugene,Eugene; You have a couple of patches attached. 2 of them saying ZOOKEEPER-1103. Is ZOOKEEPER-1104 patch the one to consider for this jira?,+1 overall. Here are the results of testing the latest attachment http://issues.apache.org/jira/secure/attachment/12487200/ZOOKEEPER-1104.patch against trunk revision 1152141. +1 @author. The patch does not contain any @author tags. +1 tests included. The patch appears to include 4 new or modified tests. +1 javadoc. The javadoc tool did not generate any warning messages. +1 javac. The applied patch does not increase the total number of javac compiler warnings. +1 findbugs. The patch does not introduce any new Findbugs (version 1.3.9) warnings. +1 release audit. The applied patch does not increase the total number of release audit warnings. +1 core tests. The patch passed core unit tests. +1 contrib tests. The patch passed contrib unit tests. Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/449//testReport/ Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/449//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/449//console This message is automatically generated.,-1 overall. Here are the results of testing the latest attachment http://issues.apache.org/jira/secure/attachment/12487200/ZOOKEEPER-1104.patch against trunk revision 1148553. +1 @author. The patch does not contain any @author tags. +1 tests included. The patch appears to include 4 new or modified tests. +1 javadoc. The javadoc tool did not generate any warning messages. +1 javac. The applied patch does not increase the total number of javac compiler warnings. +1 findbugs. The patch does not introduce any new Findbugs (version 1.3.9) warnings. -1 release audit. The applied patch generated 26 release audit warnings (more than the trunk's current 24 warnings). -1 core tests. The patch failed core unit tests. +1 contrib tests. The patch passed contrib unit tests. Test results: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/404//testReport/ Release audit warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/404//artifact/trunk/patchprocess/patchReleaseAuditProblems.txt Findbugs warnings: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/404//artifact/trunk/build/test/findbugs/newPatchFindbugsWarnings.html Console output: https://builds.apache.org/job/PreCommit-ZOOKEEPER-Build/404//console This message is automatically generated.,sorry should have read 'since June 21-June 22' {{ commit 9619bd31d5b191fdb7d622d2bb84fac92ab1111c Author: Patrick D. Hunt <phunt@apache.org> Date: Wed Jun 22 19:35:55 2011 +0000 Fixed a problem introduced by the first patch for ZOOKEEPER-1103 (phunt) git-svn-id: https://svn.apache.org/repos/asf/zookeeper/trunk@1138595 13f79535-47bb-0310-9956-ffa450edef68 commit 77992edd8abc6ffd759d60266e55fd3b388b7d3a Author: Patrick D. Hunt <phunt@apache.org> Date: Tue Jun 21 22:19:04 2011 +0000 ZOOKEEPER-1103. In QuorumTest; use the same 'for ( .. try { break } catch { } )' pattern in testFollowersStartAfterLeaders as in testSessionMove. (Eugene Koontz v git-svn-id: https://svn.apache.org/repos/asf/zookeeper/trunk@1138213 13f79535-47bb-0310-9956-ffa450edef68 }},Hi; an earlier version of the patch for this issue is now in trunk since: I am adding another; smaller patch that syncs trunk with the modifications of this patch (removing the unneeded 'boolean success' variable).,,,,,,,,,,,,,,,
26594,,,,,,,,Close all resolved issues for Engine 1.5 release.,Thanks for giving in; Nathan and making the patch. I agree with Henning's comment in Velocity-193 that relying on finalize is not perfect; but it should be a big improvement. Note that I added a line to set appender to null; allowing shutdown to be called multiple times with no effect.,before/just-in-case you ask... apply this and then there should be no question about resolving the bug.,Yes; and no. This bug affected both Log4JLogSystem and the deprecated SimpleLog4JLogSystem. the patch for VELOCITY-403 fixes the bug in Log4JLogChute (and thereby Log4JLogSystem) and uses Log4JLogChute in the default velocity.properties instead of SimpleLog4JLogSystem. so; the bug will be fixed for almost everyone. however; anyone outdated enough to have explicitly specified SimpleLog4JLogSystem (which has been deprecated for eternity and will now generate deprecation warnings at init-time) will not have the bug fixed for them. it would be easy to fix SimpleLog4JLogSystem; but frankly; i didn't care to. now it's just one more reason to stop using an ancient class.,Was this fix included in VELOCITY-403? It doesn't look like it. just checking.,I was just reading http://issues.apache.org/jira/browse/VELOCITY-193 and discovered that Daniel's patch also should include a call to logger.removeAppender(appender); in the shutdown() method.,Daniel's patch suggestion looks good to me. There's no reason to be closing other appenders attached to the logger. Of course; it will need to be regenerated if/after my patch for VELOCITY-403 is committed. i'll happily do that if no one beats me to it.,The SimpleLog4JLogSystem is deprecated; but should still be fixed. This problem also applies to the replacement class Log4JLogSystem. How about a patch like this?: * src/java/org/apache/velocity/runtime/log/Log4JLogSystem.java åÊåÊappender: Expose to shutdown(). åÊåÊinternalInit(String): Use this.appender. åÊåÊshutdown(): Close only our appender. Index: src/java/org/apache/velocity/runtime/log/Log4JLogSystem.java =================================================================== --- src/java/org/apache/velocity/runtime/log/Log4JLogSystem.java (revision 76173) +++ src/java/org/apache/velocity/runtime/log/Log4JLogSystem.java (working copy) @@ -55;6 +55;12 @@ åÊ åÊåÊåÊåÊåÊ/** åÊåÊåÊåÊåÊåÊ* <a href='http://jakarta.apache.org/log4j/'>Log4J</a> + * file appender for our {@link #logger}. + */ + private RollingFileAppender appender = null; + + /** + * <a href='http://jakarta.apache.org/log4j/'>Log4J</a> åÊåÊåÊåÊåÊåÊ* logging API. åÊåÊåÊåÊåÊåÊ*/ åÊåÊåÊåÊåÊpublic Log4JLogSystem() @@ -127;7 +133;7 @@ åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊ*/ åÊåÊåÊåÊåÊåÊåÊåÊåÊlogger.setLevel(Level.DEBUG); åÊ - RollingFileAppender appender = new RollingFileAppender( + appender = new RollingFileAppender( åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊåÊåÊåÊåÊåÊåÊåÊnew PatternLayout( '%d - %m%n'); logfile; true); åÊåÊåÊåÊåÊåÊåÊåÊåÊ åÊåÊåÊåÊåÊåÊåÊåÊåÊappender.setMaxBackupIndex(1); @@ -177;10 +183;8 @@ åÊåÊåÊåÊåÊ/** Close all destinations*/ åÊåÊåÊåÊåÊpublic void shutdown() åÊåÊåÊåÊåÊ{ - Enumeration appenders = logger.getAllAppenders(); - while (appenders.hasMoreElements()) + if (appender != null) åÊåÊåÊåÊåÊåÊåÊåÊåÊ{ - Appender appender = (Appender)appenders.nextElement(); åÊåÊåÊåÊåÊåÊåÊåÊåÊåÊåÊåÊåÊappender.close(); åÊåÊåÊåÊåÊåÊåÊåÊåÊ} åÊåÊåÊåÊåÊ},Why create own appender in internalInit method when you can Log4J do to that. I modified internalInit method and get rid of these finalize and shutdown methods and now there are no errors. Here is code snipped from internalInit method: java.util.Properties props = new java.util.Properties(); String ap = 'log4j.appender.R'; /* * Priority is set for DEBUG becouse this implementation checks * log level. */ props.put('log4j.logger.'+this.getClass().getName(); Priority.DEBUG.toString()+ ' ;R'); props.put(ap; RollingFileAppender.class.getName()); props.put(ap+'.File'; logfile); props.put(ap+'.MaxFileSize'; '100KB'); props.put(ap+'.MaxBackupIndex'; '1'); props.put(ap+'.layout'; PatternLayout.class.getName()); props.put(ap+'.layout.ConversionPattern'; '%d - %m%n'); org.apache.log4j.PropertyConfigurator.configure(props); logger = Category.getInstance(this.getClass().getName()); // logger.setAdditivity(false); /* * Priority is set for DEBUG becouse this implementation checks * log level. */ // logger.setPriority(Priority.DEBUG); // // RollingFileAppender appender = new RollingFileAppender( new PatternLayout( '%d - %m%n'); logfile; true); // // appender.setMaxBackupIndex( 1 ); // // appender.setMaximumFileSize( 100000 ); // // logger.addAppender(appender);,I agree we are doing the wrong thing and will stop. However; not sure if we want to take that strategy for shutdown - we can try.,,,,,,,,,,,,,,
58877,,,,,,,,I haven't seen any bad behavior. I was using open ssh to test this. I used the escape character; ~; and the period character; .; with open ssh to disconnect; '~.'.,Committing to https://svn.apache.org/repos/asf/mina/sshd/trunk ... M sshd-core/src/main/java/org/apache/sshd/server/session/ServerSession.java M sshd-core/src/test/java/org/apache/sshd/ClientTest.java Committed r1027569,I've added a junit test in ClientTest to try reproducing the behavior but everything seems to work fine. Which client to do use ? I wonder if the client is not in a state to answer the channel close methods; maybe the server will wait forever.,Have you seen any bad behavior with the above patch ?,No; it doesn't for me. I thought that was purposeful although I didn't read the spec to verify. In ServerSession.java; close(false) is called which doesn't cause Command.destroy() to be called. If I changed that to close(true); then destroy is called. protected void handleMessage(Buffer buffer) throws Exception { 118 SshConstants.Message cmd = buffer.getCommand(); 119 log.debug('Received packet {}'; cmd); 120 switch (cmd) { 121 case SSH_MSG_DISCONNECT: { 122 int code = buffer.getInt(); 123 String msg = buffer.getString(); 124 log.info('Received SSH_MSG_DISCONNECT (reason={}; msg={})'; code; msg); 125 close(false); 126 break; 127 },When the server receives the SSH_MSG_DISCONNECT on a session; it closes down all channels; and the currently executed command should have its destroy() method called. Isn't that the case for you?,,,,,,,,,,,,,,,,,,
18736,,,,x,,,,You miss some logic: var e = Wicket.$(id); available++; if (e === null) { i.e. if there is an entry for a date picker in the DOM but there is no HTML element for it then the DOM entry is deleted too.,,,,,,,,,,,,,,,,,,,,,,,
73230,,,,x,,,I go for 'anger' as he disliked the initial naming,resolved -- changed DEFAULT_TTL to defaultTtl to reflect it not being a constant any more..,Code review at: http://codereview.appspot.com/185041,,,,,,,,,,,,,,,,,,,,,,
27430,,x,,,,,,Looks solid. I'm okay with resolving this one.,,,,,,,,,,,,,,,,,,,,,,,
12357,,,,x,,,,You are mixing runtimes. You must link _all_ executables with the same run- time library. For release; it's Multithreaded DLL; for debug; it's Debug Multithreaded DLL. Otherwise; bad things will happen when memory allocated from one heap is freed in another. I rebuilt your code with the appropriate setting; and everything worked fine. See the project settings dialog box; C/C++ tab; and the 'Code generation' category.,,,,,,,,,,,,,,,,,,,,,,,
39032,,,,,,,,Reopening issues to set version.,Code moved into tiles-request.,,,,,,,,,,,,,,,,,,,,,,
79309,,,,,,,,Closing old resolved issues,,,,,,,,,,,,,,,,,,,,,,,
8813,,,,,,,,checked in,This update to CellBridge.js and TabBridge.js fixes the issue.,,,,,,,,,,,,,,,,,,,,,,
77011,,,,,,,,Closing all resolved/fixed issues of already released versions of Roller.,i applied a fix for this one in roller trunk revision 530919. turns out the roller menu setup supported a way to do this; but it was not easy to see that since the menu functionality is largely undocumented and a bit confusing :/,,,,,,,,,,,,,,,,,,,,,,
46121,,,,,,,,----------------------------------------------------------- This is an automatically generated e-mail. To reply; visit: https://reviews.apache.org/r/1023/ ----------------------------------------------------------- (Updated 2011-07-08 20:04:37.543693) Review request for Thrift. Changes ------- Adding bug number for tracking. Summary ------- Remove bogus include. TAsyncChannel includes TTransportUtils but doesn't need/use it. This addresses bug THRIFT-1231. https://issues.apache.org/jira/browse/THRIFT-1231 Diffs /lib/cpp/src/async/TAsyncChannel.h 1143615 Diff: https://reviews.apache.org/r/1023/diff Testing ------- make check in lib/cpp Thanks; Diwaker,Integrated in Thrift #188 (See https://builds.apache.org/job/Thrift/188/) THRIFT-1231: Remove bogus include Client: c++ Patch: Diwaker Gupta Removing unused import TTransportUtils.h from TAsyncChannel.h. jfarrell : http://svn.apache.org/viewvc/?view=rev&rev=1144395 Files : /thrift/trunk/lib/cpp/src/async/TAsyncChannel.h,Committed; Thanks for the patch.,,,,,,,,,,,,,,,,,,,,,
15801,,,,,x,,,Given the dependencies; I don't think this is going to be an option for the cannonical Struts 2 distribution; unless it is reworked to use iBATIS or Cayenne.,I've punted this off to 2.3; or maybe 2.2.1. Not happy about it; but we need a release pronto.,Jason; you're worthless!,Jason; you're a slacker,Jason; Can you take a stab at this? I've checked in the code and started the conversion; but the switch to Spring is probably something you should do instead of me. As always; use QuickStart to launch the wwia app. (This effort has the added benefit of being the place where we point our readers to go to for updated code),Patrick; I think; we can use another name for this project rather than same name with hibernate.org project; even your book explain about this. how about add jasper also there Our project here; work smiliar with yours ceveatemptor. http://cimande.dev.java.net. We are writing a book to promote it also; we use WW 2.1.7; HB2.1.6; and Velocity. take a look of our newest version 0.7.2. there is jasper 0.5 work well; but still working in jasper 1.0.1. our focus is easy module management for this. and I think your example can be focus on integration and develop a good persistance that work very cool with WW. we will make our project depend with your project; this is 4 years old project; before that we host in http://www.sf.net/projects/cimande. Jan 2001 and our project license is Apache. and your ceveatemptor is LGPL; but we can replace it. isnt it; Frans,,,,,,,,,,,,,,,,,,
154,,,x,,,,,Marshall; I'm wondering if the bug has something to do with the following: when you create a client; the second parameter to the constructor determines the random seed used by the client. That second parameter I think is supposed to be the client's id; but what is passed is clients.size(). I might be wrong here but I don't see where this vector of clients is cleared in between tests; so is it true that at some point the set size is fixed and all the seeds are the same ?,In the equivalent Java client test; the essential idea is to use a known seed of 1; but instead of using the random numbers directly from that; it passes the numbers generated into the client's invidiaul random number generators to be the seeds.,,,,,,,,,,,,,,,,,,,,,,
75901,,,,,,,,Refactoring out common code into a parent processing class for ack request/close/terminate,Patch for storing the close message,The first patch is to store the terminate message,,,,,,,,,,,,,,,,,,,,,
41140,x,x,,,,,,Looks fine to me now. Thanks!,Ok; documentation has been updated and tomahawk site republished. See: * change to 'Extensions Filter' page * change to t:inputFileUpload tagdoc * change to ExtensionFilter class javadoc Please check and then close as 'resolved'. Thanks for pointing the issue out.,Yep; thanks for pointing that out. I'll fix that soon..,,,,,,,,,,,,,,,,,,,,,
33812,,,,,,,,Here is a complete patch removing Composite* from the getImplementation() calls,,,,,,,,,,,,,,,,,,,,,,,
2091,,,,,,,,Edited these issues to set the fix version.,,,,,,,,,,,,,,,,,,,,,,,
79349,x,,,,,,,Closing old resolved issues,Thanks William to report this. I updated the page and I will verify later today that other instructions are correct,,,,,,,,,,,,,,,,,,,,,,
4169,x,,,,,,,I'd be okay with an enhancement which defaults to a SAX2-compliant parser when available; but this isn't always the case. MinML is a good fallback for those situations (e.g. applets).,First of all; let me say; that I am not so keen with MinML either and share your opinion; that a modern XML parser and preferrably the JRE's parser should be default. However; as already pointed out; this can easily be achieved and you are by no means forced to use MinML. Besides; take a look at http://cvs.apache.org/viewcvs.cgi/ws-xmlrpc/?only_with_tag=b20050512_streaming and you'll note; that the use of SAX2 is already implemented in a branch.,A quick perusal of the MinML web page shows that it advertises non-conformance to XML. In particular it fails to handle the internal DTD subset as required by the XML specification. I don't doubt that I could find other non-conformances if I actually started parsing documents with it.,This is a very vague observation; with a very strongly worded proposal for its resolution. Might I suggest that the way to procede is to describe some concrete instances of MinML's failure in the role; and if possible provide some simple reproducable test cases to support your opinion. Might I also point out: 'http://ws.apache.org/xmlrpc/parser.html' which describes how to use any other parser. I would respectfully suggest this issue be downgraded to 'Minor' 'Wish' and resolve by changing the _default_ parser to the J2SE default.,,,,,,,,,,,,,,,,,,,,
60566,,,,,,,,[branch_4x commit] Michael McCandless http://svn.apache.org/viewvc?view=revision&revision=1390137 SOLR-3879: don't ship servlet-api*.jar,,,,,,,,,,,,,,,,,,,,,,,
14704,,,,,,,,This has been committed in XWork and will be part of the XWork 2.1 release. See: http://jira.opensymphony.com/browse/XW-541,Just realized that ActionSupport is part of XWork and not Struts 2 and I can't commit changes to XWork so I'm unassigning this to myself.,,,,,,,,,,,,,,,,,,,,,,
70304,,,,,,,,Fine. So I resolve this with just upgrading the used SLF4J API,>So can we then resolve this with just upgrading the exported SLF4J API and without the spi and helpers export ? I think so,So can we then resolve this with just upgrading the exported SLF4J API and without the spi and helpers export ?,Both mina and karaf have now been fixed.,I get it now; I wasn't seeing the spi dependency; but it's transitive via helpers. In any case; mina is fixed in trunk (2.0.0-RC2-SNAPSHOT) so that it no longer depends upon either spi or helpers.,The jar included with servicemix 4.2.0 is mina-core-2.0.0-RC1.jar. I haven't modified it at all.,Jason - what version of mina are you using? trunk depends upon org.slf4j.helpers; not org.slf4j.spi.,Reverted the export of the slf4j.helpers and slf4j.spi packages in Rev. 995563.,Reopening to resolve this veto.,-1 on exporting spi and helpers. These packages are for implementers of SLF4J; not API consumers. The only exports from commons log should be logging APIs. If we decide to expose an SPI; that should be done in a Sling package and then mapped internally to the bundle to the SPI of whatever implementation we happen to be using at the time.,Thanks for the information. I have applied your patch in Rev. 995520.,I moved spi/helpers to the private packages header again and got the following bundle problems: org.apache.felix.karaf.features.core [14]: package; (&(package=org.slf4j.helpers)(version>=1.4.0)(!(version>=2.0.0)))) org.apache.mina.core [25]: package; (&(package=org.slf4j.spi)(version>=1.5.0))) org.apache.felix.karaf.shell.log [30]: package; (&(package=org.ops4j.pax.logging.spi)(version>=1.4.0)(!(version>=2.0.0)))) (this still persists when I move the headers back because it's a dependency on pax; which I removed) I'm using servicemix 4.2; which uses karaf 1.4.0. I know this is a pretty old version of karaf; but I've been pretty hesitant to switch to 2.0.0 because of all the unknown regression issues that might pop up.,Thanks for providing the patch. I agree with updating to the 1.5.11 version of slf4j API (and helper bundles). But I am a bit unsure about exporting the spi and helpers packages. Do you have bundles importing these packages ? and to what avail ?,,,,,,,,,,,
18121,,,,,,,,Resolved with no further action; so closed.,,,,,,,,,,,,,,,,,,,,,,,
63811,,,,,,,,commited a while back,Can this issue be closed?,updated patch that includes tests and better javadocs describing when each call is safe to make. This also adds a function: refreshAnalyzers() that reloads the anylyzers. This is necessary if you modify the schema fields returned from getFields() This does not add any synchronization; it just relies on things being called within inform(),If the fields are made volatile ; should it be ok? How expensive are volatile fields? I think adding copyFields should (for now) be restricted to use before or during the inform() phase. I would g w/ this approach.Making reads expensive is not an option.,Noble - I don't think your variation is thread safe. I don't believe it can be without some level of synchronization or volatile on the read-side. Since a truly thread-safe implementation requires slowing down the read-side; I think adding copyFields should (for now) be restricted to use before or during the inform() phase.,a threadsafe version of the same,This patch has thread safety issues if the new API were to be used from more than one thread; or concurrently with searching.,This patch moves the copyField registration out of a for loop and into a public function. The downside to this approach is that the dynamicCopyField array is resized for each new dynamic copy field rather then building a List; then converting to an array. Since this is only at startup and is likely an unmesuarable change (unless you have LOTS of dynamic copy fields); this seems like an ok tradeoff.,,,,,,,,,,,,,,,,
74032,,x,,,,,,Awesome. I actually did find two more non-code files - the readme docs in java/social-api/ and have added them as deletes to my client. Good to know it checks out with you though. Unless anybody objects I'll submit this relatively soon.,,,,,,,,,,,,,,,,,,,,,,,
60669,,,,,,,,committed to 4.x and 5.x,I'd go as far as to strongly recommend people use string as the type of their ID field. Some of the ways I've thought about doing cloud in conjunction with nested documents and document blocks involves encoding sub-children as The original idea around letting people specify their own uniqueKey field and fieldType was that you could design a Solr schema that would be compatible with a random lucene index. I think that's rapidly becoming an outdated idea; and not worth the issues it causes.,,,,,,,,,,,,,,,,,,,,,,
4694,,,,,,,,As Wing Yew said; the .xsb files are not compiled Java code; so we can't generate sources for them.,,,,,,,,,,,,,,,,,,,,,,,
43512,,,,x,,,,I think you have enable ajax. f:loadBundle has only request scope. If you request the table sort the ajax response didn't know anything about the resource bundle. Please use tc:loadBundle it has session scope. But tc:loadBundle has a different semantic. Please consult the demo or other examples how to use it.,,,,,,,,,,,,,,,,,,,,,,,
68254,,,,,,,,I forgot servicemix-soap on the 3.2 branch: http://svn.apache.org/viewvc?diff_format=h&view=rev&revision=657762 And also added it to the trunk http://svn.apache.org/viewvc?diff_format=h&view=rev&revision=657764 http://svn.apache.org/viewvc?diff_format=h&view=rev&revision=657765,Fixed for 3.2 branch in http://svn.apache.org/viewvc?diff_format=h&view=rev&revision=657749 http://svn.apache.org/viewvc?diff_format=h&view=rev&revision=657755,,,,,,,,,,,,,,,,,,,,,,
7461,x,,,,,,,I verified the fix using the xml-xerces_20040107172932.tar.gz CVS snapshot. Thanks!,I've just put a fix for this in CVS; can you please verify? As to why we moved from fopen to open; see bug 4556 (http://issues.apache.org/bugzilla/show_bug.cgi?id=4556).,,,,,,,,,,,,,,,,,,,,,,
41962,,,,,,,,I have fixed that issue by checking for a null before the line 95 of HtmlInputTextRenderer: writer.writeAttribute(HTML.VALUE_ATTR; value; JSFAttr.VALUE_ATTR); 'value' is null here when using jetty+JSF-RI.,,,,,,,,,,,,,,,,,,,,,,,
22447,,,,,,,,A RangeValidator exists by now which looks very similar to yours.,,,,,,,,,,,,,,,,,,,,,,,
75638,,,,x,,,"I put ""anger"" because of ""... is still not stable""",Created an attachment (id=13928) Second version New version; major changes: åÊ* easier to use API(see tests for examples); in enveloping signatures no other parameters need to be given. åÊ* verification of enveloped signature. Stills: åÊThe api is still not stable; only verification not creation API. More ways of telling what to verificate and how.,Created an attachment (id=13738) First Version,,,,,,,,,,,,,,,,,,,,,,
4223,,,,,,,,Updated in 1.2 branch. David; please download and try 1.2 beta when it is released in a week or so.,This will be resolved when Bug 16383 is resolved. Marking as a duplicate. *** This bug has been marked as a duplicate of 16383 ***,ISO-8859-1 is the default encoding of the XmlWriter class inside org.apache.xmlrpc.XmlRpc class; but the XmlWriter class inside org.apache.xmlrpc.applet.SimpleXmlRpcClient is not. Hence the XML file generated by an applet is different from the XML file generated from a standalone application. Hope this make the issue clearer. Cheers.,Why?,,,,,,,,,,,,,,,,,,,,
31936,,,,,,,,Closing because this has been in RESOLVED state for over one year; if it turns out to not be fixed please reopen.,Thanks for ticking the box Jun Guo,Update this patch again with more appropriate attachment license 'Grant license to ASF for inclusion in ASF works'.,The problem I saw was that the test didn't throw the exception that was expected and hence the test failed. I didn't look into the details but if you don't see the same behviour lets look at what's different between your environment and my environment,Simon; Could you paste your problems here? Thanks.,Thanks for the patch Jun Guo it looks good. I committed it at revision: 718951. I marked ASM50001 as ignored as follows; åÊåÊåÊåÊ@Ignore('TUSCANY-2675') as I had problems with it. Can you take a look and see if this is a Tuscany or test problem?,Update some files. Pls use this one. The directory level is Tuscany\vtest\assembly\component\src.,I created this patch on the vtest\assembly\component level; you can try it for a verification.,Hi JunGoa Thanks for the patch. Can you try the diff from the top level of your test directory so that all the changes; new files and changes; are included in the same patch. I can then apply it in one go and test it. Thanks Simon,,,,,,,,,,,,,,,
57316,,,,,,,,Fixed with the referenced change.,This doesn't appear to affect 4.1.3.,,,,,,,,,,,,,,,,,,,,,,
34559,,,,,,,,Lazy session associated has been implemented in the TuscanyValve The spec people are still discussing Scope so I am going to close this and we can open a new issue when the spec clearly defines how scopes operate.,Moving to the correct component.,Do you mean provide integration for session event notification? There is a servlet filter that does this which has code that could be re-used. It actually does lazy-access of session id's so a session is not created if session-scoped services are not accessed during a request. Another touchpoint could be for state replication. I think we should look at a general change-tracking mechanism for the runtime internals (possible based on SDO using a binary diff format) to do this as well as implement a 'Tuscany' binding.,,,,,,,,,,,,,,,,,,,,,
24937,,,,,,,,Committed in rev 983396,,,,,,,,,,,,,,,,,,,,,,,
17369,,,,x,,,,Marking this issue as resolved. Reasons: - Not reproducible. - No feedback from reporter. - If there was a dependency issue; it has probably been fixed by the change in r773502.,Can you try this again after doing a mvn dependency:purge-local-repository?,,,,,,,,,,,,,,,,,,,,,,
12834,,,,,,,,Integrated in Struts2 #551 (See https://builds.apache.org/job/Struts2/551/) WW-3914 solves problem with returning always system implementation of FileManager (Revision 1405930) Result = SUCCESS lukaszlenart : Files : /struts/struts2/trunk/xwork-core/src/main/java/com/opensymphony/xwork2/util/fs/DefaultFileManagerFactory.java /struts/struts2/trunk/xwork-core/src/test/java/com/opensymphony/xwork2/util/fs/DefaultFileManagerFactoryTest.java,Done,,,,,,,,,,,,,,,,,,,,,,
53284,,,,,,,,Resolved in Version 1.2_QA_B1 . Therefore will be closing the issue,,,,,,,,,,,,,,,,,,,,,,,
29404,,,,,,,,To be consistent with the preferred syntax in JIRA-1146; we will adopt the following syntax: <remoteAnalysisEngine remoteReplyQueueScaleout='nn1' > .... </remoteAnalysisEngine> I will start to do the work based on the above syntax if there is no objection _x0089_ÛÒ Tong,This change requires a change to the deployment descriptor. This change adds to the <replyQueue ...> the attribute concurrentConsumers='nnn'; and deprecates the location=[local | remote] for remote delegates. The change being done is to always use remote reply queues for remote delegates (see discussion above). Change dd2spring.xsl and also change the documentation for uima-as to correspond to this change.,Testing after the patch is done. Work properly.,For testing,Applied patches; Tong; please test.,I think Eddie is probably right that increasing the number of listeners should eliminate my need to set a prefetch > 0. A thread that's being used to execute prefetch isn't doing anything that an additional listener thread wouldn't also do; as far as I understand it. So I'm +1 to proceed with the plan as Eddie outlined it.,Thanks; Eddie. Let me clarify my statements a bit ... >>>The capability to add listeners for a remote reply queue should give equal or better performance than setting a prefetch value in most cases. Can we see if a single tuning parameter is enough before adding more complexity? Note that prefetch is not part of the JMS standard and is not available in all JMS implementations. +1 to avoiding more tweaking/tuning parameters whenever possible; in favor of something that just always works well. >>>> are there use cases where the remote delegate would be able to get requests from the remote broker; but possibly not be able to send it replies? (e.g.; due to firewall issues?) >>>Yes; when the client is behind a firewall; but the remote delegate [service] and it's broker are outside the firewall. This is one of the motivations for always allocating the reply queue on the service's broker (the other main one being to eliminate the need for a colocated broker to instantiate a local reply queue; again something not possible with many JMS implementations). Well; I meant the other way 'round. So let me try again - are there use cases where the remote broker serves the remote delegate new work OK; but for some reason can't host the reply queue? >>>>>There should only ever be one listener pulling messages off of the reply queue. >>>It is sometimes desirable to have more than one thread doing deserialization of reply messages; which is the original point of this issue. Yes; true - I meant one listener process (perhaps with mutliple threads though). So there's no need to worry about 'load balancing' due to prefetch > 0 - or is there? Not sure how these threads for multiple concurrent listeners interact with prefetch. >>>>>>>Is there a penalty for setting up the prefetch value to something high? >>>>The main problem for UIMA is memory management; as serialized XmiCas messages can be quite large. So this; together with comment above about prefetch and testing if it's needed tell me the decision to support prefetch is not yet figured out... but that we should lean toward avoiding adding this complexity if it can be shown it isn't needed. Adam - can you do a test or offer an opinion here?,The capability to add listeners for a remote reply queue should give equal or better performance than setting a prefetch value in most cases. Can we see if a single tuning parameter is enough before adding more complexity? Note that prefetch is not part of the JMS standard and is not available in all JMS implementations. >are there use cases where the remote delegate would be able to get requests from the remote broker; but possibly not be able to send it replies? (e.g.; due to firewall issues?) Yes; when the client is behind a firewall; but the remote delegate [service] and it's broker are outside the firewall. This is one of the motivations for always allocating the reply queue on the service's broker (the other main one being to eliminate the need for a colocated broker to instantiate a local reply queue; again something not possible with many JMS implementations). >There should only ever be one listener pulling messages off of the reply queue. It is sometimes desirable to have more than one thread doing deserialization of reply messages; which is the original point of this issue. >Is there a penalty for setting up the prefetch value to something high? The main problem for UIMA is memory management; as serialized XmiCas messages can be quite large.,Good points; Adam; to consider. Here are a couple of others (from reading our docs): are there use cases where the remote delegate would be able to get requests from the remote broker; but possibly not be able to send it replies? (e.g.; due to firewall issues?) Would it be possible to set the prefetch value for reply queues to be some fixed number (e.g.; 1 or 10 or ... ?) There should only ever be one listener pulling messages off of the reply queue. Is there a penalty for setting up the prefetch value to something high?,If you require the reply queues to be remote; please also allow me to set a prefetch value for them. I usually use a prefetch of 0 on my aggregate AS services; for better load balancing; but in some tests I've run; I found that local reply queues are faster; presumably because when a thread is ready to deserialize a CAS; it doesn't have to sit idle while retrieving the xmi from the broker. I'd like to be able to set a higher prefetch value (at least 1) for the reply queue. Speaking of which; prefetch needs to be settable in the DDE.,The deployment descriptor currently has: <remoteAnalysisEngine key='key name'> <!-- 0 or more --> . . . <replyQueue location='[local|remote]'/><!-- optional--> Some possibilities: a) if the location= is specified and is 'local'; give a message 'local reply queue no longer supported' b) We could force the local option to be remote - would this be a good idea? or should we just give an error message and force the user to update the descriptor? c) To support specifying tne number of concurrant consumers of the reply q; I propose adding an attribute to the replyQueue: 'concurrantConsumers' with a defualt value of 1. I'll proceed with this design / impl with the error message if the user says 'local'; unless other voices speak up,Along with this change; all reply queues for remote delegates will be deployed on the same broker as the delegate's input queue. Using remote reply queues for remote delegates will simplify message memory planning for a service; simplify the documentation and the CDE interface to the deployment descriptor.,,,,,,,,,,,,
65661,,,,,,,,Fixed in http://svn.apache.org/viewvc?view=revision&revision=1210845,,,,,,,,,,,,,,,,,,,,,,,
7577,,,,,,,,patch in cvs. Could you please verify?,Created an attachment (id=7891) Proposed patch for bug 22565.,,,,,,,,,,,,,,,,,,,,,,
49357,,,,,,,,patches applied,,,,,,,,,,,,,,,,,,,,,,,
36622,,,,,,,,Proposed patch.,Moving this out to later; not sure its possible to change this without breaking backwards compatibility. Please move back to 2.1.2 if you disagree.,In addition to INKfread() mentioned above; these are also candidates for getting their prototypes fixed: inkapi void *_INKmalloc(unsigned int size; const char *path); inkapi void *_INKrealloc(void *ptr; unsigned int size; const char *path); inkapi char *_INKstrdup(const char *str; int length; const char *path); inkapi void _INKfree(void *ptr); inkapi int INKfwrite(INKFile filep; const void *buf; int length); inkapi char *INKfgets(INKFile filep; char *buf; int length); inkapi INKVIO INKVConnRead(INKVConn connp; INKCont contp; INKIOBuffer bufp; int nbytes); inkapi INKVIO INKVConnWrite(INKVConn connp; INKCont contp; INKIOBufferReader readerp; int nbytes); inkapi INKReturnCode INKVConnCacheObjectSizeGet(INKVConn connp; int *obj_size); inkapi int INKVIONBytesGet(INKVIO viop); inkapi INKReturnCode INKVIONBytesSet(INKVIO viop; int nbytes); inkapi int INKVIONDoneGet(INKVIO viop); inkapi INKReturnCode INKVIONDoneSet(INKVIO viop; int ndone); inkapi int INKVIONTodoGet(INKVIO viop); inkapi INKReturnCode INKIOBufferWaterMarkGet(INKIOBuffer bufp; int *water_mark); inkapi INKReturnCode INKIOBufferWaterMarkSet(INKIOBuffer bufp; int water_mark); inkapi int INKIOBufferCopy(INKIOBuffer bufp; INKIOBufferReader readerp; int length; int offset); inkapi int INKIOBufferWrite(INKIOBuffer bufp; const void *buf; int length); inkapi INKReturnCode INKIOBufferProduce(INKIOBuffer bufp; int nbytes); inkapi const char *INKIOBufferBlockReadStart(INKIOBufferBlock blockp; INKIOBufferReader readerp; int *avail); inkapi char *INKIOBufferBlockWriteStart(INKIOBufferBlock blockp; int *avail); inkapi int INKIOBufferReaderAvail(INKIOBufferReader readerp); inkapi int INKAIONBytesGet(void* data); There were also some deprecated APIs that are 'wrong'; but we should not modify those (ever).,Any more thoughts on this bug? I think we might need to postpone changing this until 3.0; to avoid breaking APIs in the 2.x release cycle. Alternatively; we'd have to add new 64-bit variants where necessary (which seems annoying).,Should we try to 'fix' this for 2.2? Or postpone for 2.4 or 3.0?,match the other InkAPI issue TS-14,,,,,,,,,,,,,,,,,,
38950,,,,,x,,,I've re-worked my code and the problem has now 'gone away'. The original code; where the problem was found; was ported from a solution that worked fine using ADF controls. The same code resulted in this problem. I moved some controls on the page around and altered some of the attributes and the tree table now collapses and expands as expected. Quite why the re-work has now eliminated the problem I do not know but as I need to progress with other things I've decided to close this issue rather than waste anymore of anybody's time. Thanks for all your help anyway.,I think we need a testcase. I've tried reproducing this without luck in the trinidad-demo bundle; so there must be something about your configuration. For example; what are your state saving settings?,,,,,,,,,,,,,,,,,,,,,,
15330,,,,,,,,As of r438174 ; the shopping-cart is parked in the sandbox (sandbox/struts2/apps/shopping-cart) waiting to see if someone's wants to maintain it; but it hasn't been removed from the repository yet.,Resolving as won't fix; because the shopping cart example app has been removed.,,,,,,,,,,,,,,,,,,,,,,
23837,,,,,,,,Closing since Whirr was updated to jclouds 1.5.0 in WHIRR-659.,,,,,,,,,,,,,,,,,,,,,,,
21128,x,,,,,,,thanks,Here is a patch to translate the wicket-auth resources to French.,,,,,,,,,,,,,,,,,,,,,,
11881,,,,,,,,AFAIK; there is no plan to replace crimson with xerces in near future ( maybe for xerces2; but that's still far away). There are many reasons for that ( requirements related with code size and speed ). In any case; the dist from apache uses xerces as default.,To me there is one central issue - Crimson is being phased out in favor of Xerces. For this reason alone it should not be the default. It just confuses things. This is especially true for new developers. It took an hour of frustration to figure out why; when I pulled the xalan source apart and rebuilt it; Xalan wasn't working. I had to research issues that I should not have had too.,Costin should own this one. The basic plan is to remove the javax.xml.parser stuff from the Xalan jar; and let this be a parser problem. Personally; I think it's totally bogus for it to have 'org.apache.crimson.jaxp.DocumentBuilderFactoryImpl' embedded in the code; but I don't really want to be responsible for having a slightly different version of the code in Xalan than is in the JAXP 1.1 package. We'll leave the javax.xml.parser code in the repository for building; but I don't think this code should be changed.,The default factory specified in these calls is used only if no other factory specification can be found. In the case of SAXParserFactory; a factory specification is searched for as follows: 1. System property javax.xml.parsers.SAXParserFactory 2. <java.home>/lib/jaxp.properties; property javax.xml.parsers.SAXParserFactory 3. File META-INF/services/javax.xml.parsers.SAXParserFactory 4. The default: org.apache.crimson.jaxp.SAXParserFactoryImpl Xalan currently supplies a file in the META-INF subdirectory (in xalan.jar) that contains åÊåÊorg.apache.xerces.jaxp.SAXParserFactoryImpl as its contents. Do people think we should eliminate the default or change it to the Xerces ParserFactory or leave it at Crimson; the way it is now. This discussion also applies to the DocumentBuilder Factory. If there is no opposition; I agree with Pat that we should change the default to Xerces. Gary,,,,,,,,,,,,,,,,,,,,
52102,,,,,x,,,Due to limitations in JIRA; the resolution on some issues had to be changed; as part of removing the fix release from the issue. Only issues that are actually fixed should have a fix release.,MARKER: INVALID FIX RELEASE,Please open a new one for 5.3 if this still applicable,,,,,,,,,,,,,,,,,,,,,
180,,,,,,,,Jimmy this ready for review/commit? If so please 'submit'. Thanks.,Patch version 2 which supports async batch.,I am hacking around this week. I will find out how much performance I can gain this way; if any. Yes; it is mainly for performance and convenience. As to fewer zk events; it may be just for our use case; assuming our node children changed handler doesn't have a chance to reset the watch soon enough. So if we create lots of children for one parent; we may get less node children changed events; theoretically.,I had a look at the patch; and I'm not sure how you take care of the scenario you mention above of receiving fewer zk events. With this patch; wouldn't you still get as many notifications as watches you had set for the znodes you have manipulated in your batch? I'm still interested in understanding if the performance difference you claim matters or of it can be fixed some other way. This feature is mainly for performance and convenience; yes?,Attached is the first version patch which is on top of the patch for ZK-1569 and ZK-1592. This patch supports sync-batch only. The async version will be in the next patch.,Hi guys; Here are two concerns I have: With this proposal we are now mixing performance and correctness (atomicity) in the multi abstraction. At this point; I'd rather stick only to the correctness aspect. The architecture of zookeeper is essentially an execution pipeline; which has been optimized to provide both low latency and high throughput. This proposal goes in the opposite the direction and tries to promote the execution of large batches instead of individual operations at least for some use cases. In general; if it there is an opportunity to improve the performance of the system; then we should pursue it; but at this point it is not even clear how much difference it would actually make if any. Can we actually make sure that such an app-level batching makes a significant difference compared to trunk with respect to performance? And if it does; what exactly is the culprit? Can we fix it without introducing a new API feature? The point about getChildren capturing fewer events sounds like a 'good to have' but not really 'must have'; but please correct me if I'm wrong.,For our use case; there is no dependency issue. 'Batch' is what we want. Another benefit is that we will get less ZK events. This may not be obvious. But we will do get less ZK events. For example; for a bunch of create operations; if we do it one by one; once we get a nodeChildrenChanged event; we will watch it again; so we will get another one for the next create operation. If they are batched; after we get the first nodeChildrenChanged event; when we watch it again; most likely; other nodes are already created; so we will get less events; which is good.,Well obviously the way it is currently implemented we do not proceed past the first failure. But if we wanted to support a 'batch' request wherein they are not all or nothing; then yes; I think we'd proceed past the first failure. If there are dependencies on earlier ops; then obviously those will fail.,it aborts on the first op that fails and rolls back Should we allow operations after the failed operation to continue ? The rationale is that the operations in the batch may not have dependencies among them.,Yes; I meant 'cause' . The existing multi code fills in a list of results for each op. Right now; it aborts on the first op that fails and rolls back the data tree to what it was before it started. And it explicitly marks all ops after that in the results list with a runtime exception. So the mechanism is already there to communicate the errors back to the client. So I suppose the Multi code would need to take a bool to indicate if it was all or nothing or not.,A multi will case one new snapshot/log to be generated I guess you meant 'cause' above. but there was no guarantee they'd all succeed/fail. I think we need to formalize how success / failure status for individual operations in this new multi API should be delivered back to client.,I actually think there is a valid use case for this. Mostly for performance reasons. Because a multi is one transaction; it causes less permuation on the distributed and replicated state of zookeeper than multiple individual operations not in a multi. With a Multi: You only pay the cost of the RPC overhead once rather than on each individual operation You get one flush of the leader channel rather than multiple ones for each write operation A multi will case one new snapshot/log to be generated rather than multiple ones for each operation There are other reasons that make this a good reason too that are not performance based. e.g.; if it makes the programmer's job easier to use a multi with these semantics; then that's a win. In other distributed databases I've worked on; we used different terminology to disinguish between a multi op that all succeed/fail vs one that does not. We used the term 'Batch' to imply we were batching up operations but there was no guarantee they'd all succeed/fail.,Yes; that's what we are using now. It is working fine. I was thinking if there is still room for improvement.,In my view; the asynchronous API has been designed to address exactly use cases like yours. I don't think you should be suffering any severe penalty by using the asynchronous API. Have you actually tried it and had any issue with it?,Hi Flavio; for our use case; we need to create/setData hundreds/thousands of znodes. By submitting operations asynchronously; we need to do it one by one. If we can do it in batches; we can save lots of network trips.,Hi Jimmy; I'm trying to understand why submitting operations asynchronously is not sufficient for your case. Why do you need to use multi in this case?,,,,,,,,
37778,,,,,,,,The call to launchDialog must supply a component for delivery of the returnListener event. This bug is invalid.,WAR file recreating issue.,,,,,,,,,,,,,,,,,,,,,,
11526,,,,,,,,Original reporter is not available. This report has now 'timed out'.,,,,,,,,,,,,,,,,,,,,,,,
33117,,,,,,,,I've just made some observations on the tuscany-user mailing list which should appear in the mailing list archives shortly; under the thread that contains http://www.mail-archive.com/tuscany-user@ws.apache.org/msg02162.html,Kelvin; apparently there is no public maven repo that hosts these artifacts. We added the eclipse artifacts to our own repo using (something almost identical to) the maven eclipse plugin ( http://maven.apache.org/plugins/maven-eclipse-plugin/to-maven-mojo.html ),Kris; åÊåÊI downloaded and manually installed the eclipse core and osgi dependencies at the appropriate level into my maven repository. Currently I'm getting more eclipse missing classes such as IExtension; which I guess is because the eclipse artifacts have transitive dependencies which would be known if maven had installed the artifacts itself. I'm guessing you have a repository in your settings.xml file for maven that can resolve these artifacts automatically. I can continue to dig out jars and manually install them into my repo until all requirements are resolved; but if you happen to be able to point me at a maven repo to do the job automatically that would be great; thanks.,Kris; thanks for this. It's going to be a couple of days before I can get to looking at it; but I look forward to doing so.,The attachement agfasdo.tar.gz contains the full source code of our custom sdo implementation; including our data mapping extension. The included maven pom file might need some tweaking to get it to build.,I've added as much as I can to my sandbox in commit http://svn.apache.org/viewvc?rev=581184&view=rev I've not added anything from the hibernate integration archive due to the licensing issues alluded to earlier. Two of the files with header issues are part of the generic snapshot code; so they are missing from what I have committed (see the commit comment in http://svn.apache.org/viewvc?rev=581184&view=rev),I've been going through the files in preparation for putting them into the code base somewhere; and have been adding apache license headers to the files that have no headers; on the basis of the license you granted when attaching the zip files. There are about 10 files in the archives which have Agfa copyright headers in them (e.g. ExtendablePropertyAccessBuilder.java); saying that . ... // THIS IS UNPUBLISHED PROPRIETARY SOURCE CODE OF // Agfa-Gevaert Group åÊI don't feel it's right for me to remove those headers; although I guess that would be in the spirit of the submission. Could you please search for files with similar headers to that file; fix the headers as you see fit; and resubmit any files you wish to be included in the submission.,Hi; Thanks for the code. Sorry to take a while to get back to you. I have revisited this a few times to make sure I have a reasonably good feel for it before commenting. I'd love to be able to execute a test program to answer some of the question I have about the code; or at least to see some code that exercises it. Looking at the SnapshotSerializer code I think I'm OK to make the inference that the Type and Property classes used in there are your implementation classes of the SDO concepts; although of course that implementation code is not available. So; given that assumption; I understand that the implementation of your opaque snapshot representation is based on SDOs. It would be interesting to understand the issues you have encountered in using the code; for example; if there are any lossy round-trip transformations that cause problems. It would be great to work towards getting some code running inside the Tuscany code base; it would also be helpful if you could put some more words around any key design concepts and the issues you have encountered and solved; or have yet to solve. How should we proceed? Clearly the code is broken at the moment; given the missing aspects; so it's not suitable for including in the nightly build. I can put it in my sandbox; or I could set up another project under the SDO project; but not include it in the main build.,Bogdan; it's not in the code base yet. I'm looking at it now.,Are there any updates on this issue? Is this feature included in the nightly builds? Is it documented somewhere?,I didn't know about the license stuff and Hibernate. I'm sorry about that. I'll post some more stuff in the next days; so I should wait before committing this somewhere.,This seems pretty cool; in fact just yesterday on the user list another user was asking for exactly this functionality. There is an issue with the hibernate code unfortunately; it uses the LGPL license and we're not able to uses any code under that license in an Apache project (for the legal details see: http://www.apache.org/legal/3party.html). We can use the framework; just the code that directly references hiberbnate classes needs to be kept separate. Whats going to be easiest way to progress this; you say you're going to be adding more to it over the next days; should we wait for that before committing this somewhere or would it be easier to have it committed somewhere?,First attachment contains the main interfaces and implementation classes for snapshots; and mappers. Second attachment contains the hibernate integration.,,,,,,,,,,,
69146,,,,,,,,The fix currently transform the input source to a DOMSource. We could also handle differently a SAXSource.,The fact that it works for a particular implementation does not implies that it will for all. As the javadoc says the source should be a SAX or DOM one; i'll ensure that this is the case.,I think that the java doc is wrong ! Here is the code I tested and run perfectly !!! SchemaFactory factory = SchemaFactory.newInstance(XMLConstants.W3C_XML_SCHEMA_NS_URI); // Load a WXS schema; represented by a Schema instance. Source schemaFile = new StreamSource(new File('D:/ri.xsd')); Schema schema = factory.newSchema(schemaFile); // Create a Validator object; which can be used to validate // an instance document. Validator validator = schema.newValidator(); validator.validate(new StreamSource(new FileReader('D:/ri.xml')));,yes - from http://java.sun.com/j2se/1.5.0/docs/api/javax/xml/validation/Validator.html 'Note that while the validate(javax.xml.transform.Source) and validate(javax.xml.transform.Source; javax.xml.transform.Result) methods take a Source instance; the Source instance must be a SAXSource or DOMSource',Are you sure about this ?? The Validation class accepts a Source object. Check http://java.sun.com/developer/technicalArticles/xml/validationxpath/ for more details.,I agree; the ValidateComponent SHOULD support all the Source subclasses - but it doesn't. As I said the ValidateComponent uses a javax.xml.validation.Validator which doesn't support StreamSource.,I donå«t think that this is an issue. The StreamSource could be an XML too. I think that the ValidateComponent SHOULD support the different Source subclasses. The validation is about to decide whether the content is all right or not.,,,,,,,,,,,,,,,,,
61974,,,,,,,,bulk close for 3.4,,,,,,,,,,,,,,,,,,,,,,,
76494,,,,,,,,Closed issues related to Roller 5.0 release.,Mavenization is now complete and jetty:run-war is working on MacOS and WinXP. If you have Maven 2.0.9 or later and a Subversion client; it is now possible to fetch Roller source; build the code and run the unit tests with these commands: åÊåÊåÊsvn co https://svn.apache.org/repos/asf/roller/trunk roller_trunk åÊåÊåÊcd roller_trunk åÊåÊåÊmvn install If you want to run Roller for development testing; you can use this command which will start Jetty; start Derby and start Roller: åÊåÊåÊcd weblogger-web åÊåÊåÊmvn jetty:run-war Once you've done that; browse to http://localhost:8080/roller to run Roller. Note that this is for development testing only -- any changes you save in Roller will be deleted the next time you run mvn clean.,URL: http://svn.apache.org/viewvc?rev=900145&view=rev Log: Fixes to POMs suggested by Matt Raible Added some of the fixed suggested by Matt; in particular I fixed these items: * <type>jar</type> is unnecessary since it's the default. Same goes for <scope>compile</scope>. * An XSD on <project> might give IDEs better support for Maven. * I get '[WARNING] Using platform encoding (MacRoman actually) to copy filtered resources' when running 'mvn install'. Adding <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding> to the <properties> section will fix this.,Mavenized Roller is now in trunk; here is the initial commit. URL: http://svn.apache.org/viewvc?rev=895623&view=rev Log: SVN merge of Mavenized Roller seems to be complete I'm still working on reducing the number of jars in WEB-INF/lib; following Maven best practices; etc. Matt Raible made some good suggestions in this email thread: åÊåÊåÊhttp://markmail.org/message/xeaxln3otkhesjxm,I'm making good progress on this and hope to have something ready to commit in the next week. I'm breaking Roller up into these modules: åÊåÊåÊroller-core åÊåÊåÊplanet-business åÊåÊåÊplanet-web åÊåÊåÊweblogger-business åÊåÊåÊweblogger-web Currently; I have all modules except for the *-web ones building and successfully executing all tests.,,,,,,,,,,,,,,,,,,,
20061,,,,,,,,Fixed with r1097472.,Link with WICKET-3500. Seems to be the very same problem. It has been fixed only in 1.4.x,This specific bug is fixed but stripping comments causes IE conditional comments to become broken/cut. It would be good to test WICKET-3433 with getMarkupSettings().setStripComments(true);,A patch fixing the problem. Please review.,I added a unit test demonstrating the problem: org.apache.wicket.markup.MarkupParserTest.wicket3648testCommentsWithNestedElements() Remove the prefix 'wicket3648' to run it.,_¢he problem is that there is an inner tag and the RawMarkup for the opening comment has no knowledge about the closing tag.,,,,,,,,,,,,,,,,,,
14237,,,,,,,,Resolving this issue as the original problem has been fixed. Please open a new ticket if you feel session invalidation should also be addressed.,After some thought; the above solution doesn't apply to concurrent AJAX requests from a single user where session invalidation wins. But I think this is more of an application design issue and not really a flaw in the framework.,Today; I would think most session invalidation logic is as follows (as described in http://struts.apache.org/2.0.11.1/docs/how-do-we-get-invalidate-the-session.html): // Attempt to invalidate the session if (getSession() instanceof SessionMap) { try { ((SessionMap) getSession()).invalidate(); } catch (IllegalStateException e) { log.error(e); } } Couldn't we doing something where the session invalidation is deferred? So instead of ((SessionMap) getSession()).invalidate(); we have getSession() .invalidate() which will defer invalidation until the end of the request; either by FilterDispatcher or by a ServletRequestListener.,The I18N interceptor in xwork 2.1.2 is now patched. The session object is synchronized; but this is mainly xwork may use other implementations than S2 SessionMap. However; I think we need to adjust the SessionMap to take care of invalid session exceptions.,MessageStoreInterceptor and many others share the same issue. ExecuteAndWaitInterceptor and TokenInterceptor both synchronize on the session map. I don't think synchronizing on the map will stop the container from invalidating the session between the two operations above. The SessionMap implementation contains a synchronized block on the real session object within the get; put and remove methods. The purpose of the Map is abstract the session; so shouldn't the SessionMap be exclusively responsible for ensuring an invalid session exception doesn't occur?,Would that really be enough though? Just because we synchronized on the object doesn't mean that any other code calling its methods would also be synchronized. I'm not sure the best solution here without leaking the session object specifics through the nice Map abstraction. Any ideas?,,,,,,,,,,,,,,,,,,
5373,,x,,,,,,I mailed jsr-206-comments@jcp.org and got the following response from Jeff Suttor: 'hi Daniel; consistency and least astonishment are valuable. :) would you be open to filing an RFE http://bugs.sun.com or an issue http://jaxp.dev.java.net to insure this is addressed? as this would be a public API change; it will have to be part of JAXP.next. thanks; -- Jeff' I filed https://jaxp.dev.java.net/issues/show_bug.cgi?id=9,The class/method you're asking about is part of JAXP 1.3; a standard developed through the JCP. You should send your comments to the appropriate forum (i.e. jsr-206-comments@jcp.org). There's nothing that can be done about this in Xerces and given that JAXP 1.3 went final without having IOException in the throws clause of this method adding it now would be a backwards incompatible change.,,,,,,,,,,,,,,,,,,,,,,
60192,,,,,,,,[branch_4x commit] Mark Robert Miller http://svn.apache.org/viewvc?view=revision&revision=1427873 SOLR-4254: Harden the 'leader requests replica to recover' code path.,[trunk commit] Mark Robert Miller http://svn.apache.org/viewvc?view=revision&revision=1427872 SOLR-4254: Harden the 'leader requests replica to recover' code path.,,,,,,,,,,,,,,,,,,,,,,
24867,,,,,,,,Updated version of the patch. Removed the setter for the log storage provider in the stanza relay; and passed it either as constructor parameter of through provider registry as with other providers. If there is a need for a more generic stanza logger; these users could use the plain interface. The adapter I made is intended for filtering; so that only textual chat messages between users are let through; which I think is the most interesting detail to be logged. I updated the source file name and the comment to better indicate this.,It'd be cool to have a more integrated and general solution to serve as many users as possible. Do you like to work out an improved patch? Thanks for contributing to Vysper!,My implementation logs message stanzas only because I was only interested in logging chat messages sent between users. The storage provider should be looked up from the context. I can see that now.,This storage provider should interact with the StorageProviderRegistry infrastructure. Otherwise we'd end up with storage providers scattered everywhere.,Why does it only log <message> stanzas?,An attempt at implementing chat logging storage provider.,,,,,,,,,,,,,,,,,,
64407,,,,,,,,This bug was modified as part of a bulk update using the criteria... Marked ('Resolved' or 'Closed') and 'Fixed' Had no 'Fix Version' versions Was listed in the CHANGES.txt for 1.1 The Fix Version for all 38 issues found was set to 1.1; email notification was suppressed to prevent excessive email. For a list of all the issues modified; search jira comments for this (hopefully) unique string: 20080415hossman3,,,,,,,,,,,,,,,,,,,,,,,
8502,,,,,,,,I tried using your testcase on Win95/IE4; using old versions of Xerces (back until 1.5.1); but I never got the crash you report. So; I am closing this bug; if you still see it; please reopen it. Alberto,Created an attachment (id=148) MFC VC6 project,,,,,,,,,,,,,,,,,,,,,,
12188,,,,,,,,This patch disables variable scope checking in extension elements; which is the best we can do right now. Committers; can you please review?,This looks like a bug in our variable tracking code. However; you should be aware that xalan-C does not implement extension elements; so it does not implement the EXSL extension element 'function'.,,,,,,,,,,,,,,,,,,,,,,
9798,,,,,,,,closing this issue.,Closing as Won't fix; per JIRA meeting July 12; 2005.,,,,,,,,,,,,,,,,,,,,,,
23492,,,,,,,,behvaiors:ajax calls is not always 1:1. some behaviors are purely clientside. also; this comes up often so we should offer the devs a choice to do this even if it can have unwanted sideeffects.,,,,,,,,,,,,,,,,,,,,,,,
71371,,,,,,,,Tested this fix with two servlet containers (Jetty and Day Servlet Engine) as well as with Firefox 2 on Linux and Windows and Firefox 3 (Windows); IE6 (Windows) and Safari (Windows). Provided the data is submitted as a multipart/form-data POST request; the form data is correctly encoded and may be used for further perusal. Considering this issue fix.,Implemented correct re-encoding in Rev. 698147.,,,,,,,,,,,,,,,,,,,,,,
24246,x,,,,x,,,Thanks Tom; sorry for the mess.,,,,,,,,,,,,,,,,,,,,,,,
61660,,,,,,,,Variables specified in data config are also resolved with system properties so there is no reason anymore to configure DIH via solrconfig. Hence; the bug can be worked around by using dataconfig instead of configuring via solrconfig.,moving all 4.0 issues not touched in a month to 4.1,rmuir20120906-bulk-40-change,bulk fixing the version info for 4.0-ALPHA and 4.0 all affected issues have 'hoss20120711-bulk-40-change' in comment,After looking closer I see this is only happening because I am specifying the dataimport config in the dataimporthandler config in solrcore.xml. Moving that to a datasource element in data-config.xml prevents this; but makes it hard to specify the connection properties in a way that is easily set in deployment.,dataimporthandler config:   <!-- data import handler -->   <requestHandler name='/dataimport' class='org.apache.solr.handler.dataimport.DataImportHandler'>     <lst name='defaults'>       <str name='config'>data-config.xml</str>       <lst name='datasource'>          <str name='driver'>${jdbc.readonly.driver}</str>          <str name='url'>jdbc:mysql://${jdbc.readonly.host}:${jdbc.readonly.port}/${jdbc.readonly.database}</str>          <str name='user'>${jdbc.readonly.user}</str>          <str name='password'>${jdbc.readonly.password}</str>       </lst>     </lst>   </requestHandler> curl http://localhost:8066/solr/catalog/dataimport  <response><lst name='responseHeader'><int name='status'>0</int><int name='QTime'>0</int></lst><lst name='initArgs'><lst name='defaults'><str name='config'>data-config.xml</str><lst name='datasource'><str name='driver'>com.mysql.jdbc.Driver</str><str name='url'>jdbc:mysql://localhost:3306/#REDACT#</str><str name='user'>#REDACT#</str><str name='password'>#REDACT#</str></lst></lst></lst><str name='status'>idle</str><str name='importResponse'/><lst name='statusMessages'><str name='Total Requests made to DataSource'>2125</str><str name='Total Rows Fetched'>1965</str><str name='Total Documents Skipped'>0</str><str name='Full Dump Started'>2011-10-03 11:31:02</str><str name=''>Indexing completed. Added/Updated: 236 documents. Deleted 0 documents.</str><str name='Committed'>2011-10-03 11:31:35</str><str name='Optimized'>2011-10-03 11:31:35</str><str name='Total Documents Processed'>236</str><str name='Time taken '>0:0:32.932</str></lst><str name='WARNING'>This response format is experimental.  It is likely to change in the future.</str></response>  log:      [java] INFO  SearchHandler - Adding  component:org.apache.solr.handler.component.StatsComponent@f786a3c      [java] INFO  SearchHandler - Adding  debug component:org.apache.solr.handler.component.DebugComponent@2a869113      [java] INFO  DataImportHandler - Processing configuration from solrconfig.xml: {config=data-config.xml;datasource={driver=com.mysql.jdbc.Driver;url=jdbc:mysql://localhost:3306/#REDACT#;user=#REDACT#;password=#REDACT#}}      [java] INFO  DataImportHandler - Adding properties to datasource: {user=#REDACT#; password=#REDACT#; url=jdbc:mysql://localhost:3306/#REDACT#; driver=com.mysql.jdbc.Driver}      [java] INFO  DataImporter - Data Configuration loaded successfully,Can you please give me a log fragment showing this? The log files do have the jdbc url so it is advisable to use the user and password attributes on JdbcDataSource rather than putting them in the jdbc url itself.,,,,,,,,,,,,,,,,,
